setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")
library(tidyverse)

google_mobility <- read.csv("google_mobility_data.csv")

#fix date
google_mobility$date <- as.Date(google_mobility$date)



#now back to Google's mobility data. Plot all of the things in the dataframe
#first reshape the df so that it has all the in the rows
#we need new package, tidyr, and use gather() and spread() function
#see examples at the start of this code.
library(tidyr)
#filter the df based on taiwan, and the dates 2021-01-01 2021-05-18
google_mobility_TW <- google_mobility%>%
  filter(country_region == "Taiwan")%>%
  filter(date >= '2021-01-01' & date <= '2021-05-18')

l_tw_mobility <- google_mobility_TW [, -c(1:4)] %>% #delete the columns we don't need
  gather('move', 'value', 2:7)
  
  
#other practice_______________________________________________________________________________________
#https://www.projectpro.io/recipes/what-is-use-of-gather-function-tidyr-package#:~:text=Recipe%20Objective-,What%20is%20the%20use%20of%20gather%20function%20in%20tidyr%20package,forms%20the%20key%2Dvalue%20pairs.
df <- data.frame(student_name = c("A","B","C","D","E"),
                 Math_marks = c(70,56,34,89,54),
                 Sci_marks = c(35,55,87,12,63),
                 Eng_marks = c(59,89,55,63,55))
print(df)

#use gather 
df_l <- gather(df, 'subject', 'mark', 2:4)

#and put it back again. it is actually a pretty smart system that groups stuff together
df_w <- spread(df_l, subject, mark)



#week 1 and homework 1 _____________________________________________________________________________________________________

#data
#Homework 02 Dataset

#Run the following vector codes. You will have 9 vector objects.

#115th House Members' Gender

gender_115 <- c("M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "F", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "F", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "F", "F", "F", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "M", "M", "F", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "F", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "F", "F", "M", "F", "F", "F", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M")

#115th House Members' Birth Date

birth_115 <- c("1954-09-16", "1946-05-27", "1965-07-22", "1979-06-19", "1951-11-07", "1980-04-18", "1958-06-12", "1972-03-09", "1948-03-23", "1963-08-16", "1962-01-16", "1979-07-16", "1956-01-28", "1973-07-24", "1976-09-15", "1949-09-15", "1953-10-03", "1950-03-12", "1958-01-26", "1965-03-02", "1947-02-02", "1950-06-20", "1958-11-07", "1963-02-08", "1967-03-18", "1951-07-13", "1947-02-04", "1951-01-16", "1952-06-06", "1955-04-26", "1948-08-16", "1962-02-10", "1954-10-14", "1933-05-31", "1960-12-30", "1977-02-06", "1955-04-11", "1945-04-07", "1964-07-27", "1975-06-15", "1960-08-25", "1954-04-29", "1961-11-21", "1952-08-28", "1951-05-08", "1959-02-16", "1962-05-31", "1971-10-21", "1950-12-23", "1961-10-17", "1947-04-27", "1955-02-16", "1953-06-08", "1952-01-09", "1964-11-18", "1963-03-31", "1974-10-16", "1941-11-06", "1957-09-06", "1961-05-01", "1966-08-20", "1974-09-16", "1953-01-22", "1967-03-26", "1966-07-28", "1953-07-07", "1961-07-15", "1963-07-17", "1964-11-21", "1956-07-27", "1944-10-26", "1975-05-13", "1940-07-21", "1955-03-19", "1949-05-24", "1949-04-28", "1950-05-20", "1966-08-16", "1972-08-19", "1959-06-30", "1948-06-11", "1950-03-30", "1929-05-16", "1943-03-03", "1954-06-19", "1958-01-24", "1952-04-13", "1976-09-07", "1953-04-06", "1961-01-21", "1966-01-22", "1956-07-24", "1962-03-16", "1955-09-19", "1956-08-24", "1951-01-18", "1980-03-01", "1960-05-10", "1970-03-01", "1941-09-06", "1970-01-05", "1944-04-13", "1947-05-27", "1957-07-29", "1963-04-16", "1943-03-02", "1962-02-17", "1957-03-12", "1967-07-29", "1960-05-24", "1978-09-14", "1952-03-31", "1964-02-21", "1966-05-07", "1961-09-25", "1953-11-23", "1946-10-06", "1956-11-06", "1953-08-05", "1971-10-03", "1966-01-07", "1947-07-21", "1953-02-16", "1963-08-04", "1961-03-03", "1947-02-18", "1942-12-13", "1954-09-27", "1956-07-19", "1959-08-25", "1954-05-16", "1961-12-12", "1952-08-25", "1966-11-15", "1973-12-17", "1962-10-11", "1954-02-25", "1960-12-27", "1955-10-07", "1943-06-29", "1948-05-16", "1957-06-19", "1946-04-29", "1952-10-29", "1981-04-12", "1982-05-07", "1984-03-03", "1979-11-20", "1945-01-24", "1972-03-27", "1961-04-17", "1954-06-14", "1953-08-18", "1974-11-25", "1967-09-04", "1976-08-05", "1952-09-22", "1958-11-22", "1975-03-08", "1964-08-22", "1943-01-18", "1972-01-31", "1963-11-07", "1970-02-03", "1947-10-17", "1947-09-01", "1958-03-15", "1948-02-19", "1955-07-03", "1964-02-18", "1953-12-10", "1951-05-04", "1962-04-18", "1956-06-01", "1957-01-25", "1960-10-13", "1936-09-05", "1952-07-29", "1957-05-29", "1961-12-04", "1978-11-03", "1960-04-22", "1959-10-06", "1961-08-24", "1956-12-05", "1966-07-05", "1968-04-17", "1983-09-12", "1939-06-14", "1971-11-04", "1964-02-18", "1969-01-31", "1966-03-01", "1976-12-07", "1977-08-19", "1953-11-01", "1950-01-12", "1965-09-21", "1970-08-04", "1960-09-12", "1963-06-10", "1954-10-02", "1954-11-10", "1935-12-03", "1930-10-11", "1972-01-30", "1959-10-24", "1943-02-10", "1964-02-17", "1957-03-17", "1946-06-17", "1962-11-09", "1952-09-06", "1966-03-01", "1956-04-30", "1948-05-10", "1980-10-04", "1976-09-13", "1980-04-25", "1958-08-11", "1974-01-01", "1963-03-16", "1944-04-05", "1949-05-28", "1978-02-27", "1966-12-17", "1973-07-19", "1956-09-05", "1966-10-08", "1967-12-08", "1968-07-05", "1960-07-02", "1984-06-27", "1954-05-24", "1952-06-25", "1964-04-22", "1965-06-15", "1948-07-22", "1956-04-18", "1954-10-18", "1948-09-23", "1946-07-16", "1958-11-14", "1931-09-06", "1955-09-23", "1940-02-21", "1969-03-29", "1966-07-15",
               "1946-05-12", "1952-12-23", "1947-12-21", "1955-08-11", "1963-12-22", "1975-12-06", "1941-03-08", "1937-07-05", "1960-01-06", "1952-05-07", "1972-06-07", "1959-10-24", "1955-03-31", "1960-10-16", "1946-02-19", "1966-07-30", "1951-02-23", "1952-08-15", "1960-08-09", "1971-01-13", "1980-07-10", "1944-09-25", "1965-01-26", "1962-01-14", "1956-07-10", "1954-07-12", "1961-10-10", "1959-11-20", "1975-10-22", "1947-03-28", "1969-05-22", "1951-06-18", "1966-03-22", "1959-07-28", "1955-10-20", "1953-09-25", "1975-10-01", "1969-02-27", "1961-05-08", "1961-05-08", "1971-06-05", "1951-04-18", "1957-04-29", "1978-10-24", "1977-07-26", "1967-07-21", "1978-09-16", "1952-09-11", "1947-06-13", "1936-12-04", "1949-02-14", "1955-07-10", "1971-11-30", "1943-12-17", "1958-12-13", "1953-06-20", "1937-06-13", "1973-10-01", "1946-01-24", "1972-09-26", "1962-12-09", "1970-02-21", "1951-10-30", "1954-05-14", "1969-10-01", "1937-01-25", "1965-05-14", "1958-12-17", "1947-08-24", "1940-03-26", "1953-05-01", "1962-05-27", "1958-06-17", "1944-06-29", "1955-04-02", "1948-08-15", "1966-05-13", "1964-08-14", "1948-09-10", "1953-11-01", "1975-05-12", "1963-12-30", "1947-12-18", "1940-08-17", "1954-10-08", "1958-10-17", "1947-12-29", "1962-12-13", "1965-10-20", "1971-11-18", "1950-08-29", "1958-12-03", "1965-02-15", "1957-08-04", "1973-09-13", "1976-07-27", "1945-07-21", "1958-07-16", "1937-12-31", "1947-06-21", "1970-02-09", "1953-12-04", "1970-11-21", "1952-07-15", "1957-08-02", "1961-09-13", "1959-10-18", "1962-04-25", "1972-02-16", "1941-06-12", "1951-10-12", "1972-08-25", "1946-01-31", "1946-11-23", "1963-05-25", "1952-09-02", "1970-01-29", "1973-07-16", "1955-01-19", "1969-01-28", "1960-05-28", "1962-05-22", "1965-10-06", "1959-08-30", "1944-05-26", "1960-06-22", "1961-08-20", "1951-10-19", "1962-03-03", "1947-04-30", "1945-06-27", "1969-12-10", "1943-06-14", "1943-10-24", "1955-03-22", "1965-01-01", "1952-12-02", "1954-10-24", "1958-02-21", "1961-01-10", "1950-09-08", "1976-07-12", "1951-01-26", "1929-08-14", "1953-03-04", "1947-11-19", "1980-06-16", "1965-06-15", "1970-12-19", "1964-01-23", "1978-02-25", "1950-05-14", "1984-07-02", "1960-07-15", "1965-03-24", "1962-08-31", "1980-11-16", "1960-12-10", "1979-06-27", "1961-02-04", "1948-01-28", "1951-01-24", "1959-07-27", "1958-07-15", "1962-10-21", "1956-11-09", "1950-05-23", "1949-06-18", "1965-04-04", "1960-10-16", "1946-04-26", "1960-01-11", "1953-04-23", "1977-04-14", "1961-03-07", "1971-01-03", "1963-02-13", "1953-03-28", "1949-08-13", "1962-09-13", "1951-04-12", "1957-01-10", "1969-05-20", "1963-08-17", "1962-05-14", "1964-04-06", "1966-09-27", "1938-08-15", "1945-02-06", "1953-07-02", "1949-04-27", "1947-05-02", "1958-06-17", "1967-11-18", "1957-06-07", "1949-09-13", "1942-11-05", "1947-07-31", "1959-02-03", "1957-02-18", "1970-02-11", "1947-11-04", "1976-01-08", "1955-04-13", "1933-06-09", "1968-05-11", "1980-01-30", "1961-11-01")

#115th House Members' Party

party_115 <- c("R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "R", "R", "R", "R", "D", "R", "D", "D", "D", "D", "R", "D", "R", "R", "R", "R", "D", "R", "R", "R", "D", "D", "D", "D", "R", "D", "R", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "R", "R", "D", "D", "R", "R", "D", "D", "D", "D", "R", "R", "D", "D", "D", "R", "R", "R", "D", "D", "D", "D", "D", "D", "R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "D", "D", "R", "D", "D", "D", "R", "D", "R", "R", "D", "D", "D", "R", "D", "R", "R", "R", "D", "R", "D", "D", "D", "D", "D", "D", "D", "R", "R", "R", "D", "R", "D", "R", "D", "D", "R", "D", "R", "R", "R", "R", "D", "R", "D", "D", "D", "R", "D", "D", "R", "R", "R", "R", "R", "R", "R", "D", "R", "D", "R", "R", "D", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "D", "R", "R", "R", "R", "R", "D", "D", "R", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "R", "D", "R", "R", "D", "R", "R", "D", "R", "D", "R", "R", "R", "R", "R", "D", "D", "D", "R", "R", "D", "R", "D", "R", "R", "D", "R", "R", "R", "D", "R", "D", "R", "D", "R", "D", "D", "D", "D", "D", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "R", "D", "R", "R", "D", "D", "D", "R", "D", "D", "D", "R", "D", "R", "D", "D", "D", "R", "D", "D", "R", "R", "R", "D", "D", "R", "R", "D", "D", "D", "R", "D", "D", "R", "R", "R", "R", "R", "D", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "R", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "D", "R", "R", "D", "R", "D", "D", "D", "R", "R", "D", "D", "R", "D", "R", "D", "D", "R", "R", "D", "R", "D", "D", "R", "D", "R", "D", "D", "R", "D", "D", "D", "R", "D", "D", "R", "R", "D", "R", "R", "D", "R", "D", "R", "D", "R", "R", "R", "R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "R", "R", "R", "D", "R", "R", "R", "R", "D", "R", "D", "D", "D", "R", "R", "R", "D", "D", "D", "R", "D", "R", "D", "D", "D", "D", "D", "R", "D", "D", "R", "R", "D", "R", "D", "D", "D", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "R", "R", "D", "D", "R", "R", "R", "D", "D", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "D", "R", "D", "R", "R", "R", "D", "D", "D", "D", "D", "R", "R", "R", "R", "R", "R", "D", "D", "D", "D", "R", "R", "D", "R", "R", "D", "R", "D", "R", "R", "R", "R", "D", "R", "R", "R", "R", "R", "R")

#116th House Members' Gender

gender_116 <- c("M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "F", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "F", "M", "F", "F", "F", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "F", "F", "F", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "F", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "M", "F", "M", "F", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "F", "F", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "F", "M", "F", "F", "M", "F", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M")

#116th House Members' Birth Date

birth_116 <- c("1954-09-16", "1946-05-27", "1965-07-22", "1979-06-19", "1951-11-07", "1983-04-15", "1980-04-18", "1980-04-18", "1958-06-12", "1976-10-08", "1972-03-09", "1965-04-20", "1948-03-23", "1963-08-16", "1945-06-04", "1962-01-16", "1979-07-16", "1973-07-24", "1976-09-15", "1953-10-03", "1950-03-12", "1965-03-02", "1947-02-02", "1950-06-20", "1958-11-07", "1963-02-08", "1951-07-13", "1964-07-01", "1947-02-04", "1948-08-16", "1962-02-10", "1954-10-14", "1960-12-30", "1977-02-06", "1955-04-11", "1978-11-22", "1954-04-29", "1960-08-25", "1961-11-21", "1952-08-28", "1951-05-08", "1959-02-16", "1962-05-31", "1971-10-21", "1964-08-25", "1950-12-23", "1961-10-17", "1947-04-27", "1955-02-16", "1953-06-08", "1964-11-18", "1963-03-31", "1974-10-16", "1941-11-06", "1957-09-06", "1961-05-01", "1952-09-27", "1971-11-23", "1966-08-20", "1974-09-16", "1953-01-22", "1966-07-28", "1953-07-07", "1961-07-15", "1971-02-12", "1963-07-17", "1964-11-21", "1956-07-27", "1944-10-26", "1972-02-29", "1975-05-13", "1940-07-21", "1949-05-24", "1949-04-28", "1966-08-16", "1950-05-20", "1972-08-19", "1948-06-11", "1950-03-30", "1943-03-03", "1954-06-19", "1958-01-24", "1952-04-13", "1953-04-06", "1963-07-18", "1972-02-14", "1966-01-22", "1984-03-14", "1956-07-24", "1979-03-15", "1955-09-19", "1951-01-18", "1982-05-26", "1960-05-10", "1980-05-22", "1970-03-01", "1941-09-06", "1970-01-05", "1944-04-13", "1959-06-06", "1947-05-27", "1957-07-29", "1943-03-02", "1962-02-17", "1977-01-19", "1957-03-12", "1952-03-31", "1964-02-21", "1966-05-07", "1961-09-25", "1953-11-23", "1946-10-06", "1953-08-05", "1971-10-03", "1966-01-07", "1953-02-16", "1961-03-03", "1947-02-18", "1969-09-15", "1942-12-13", "1954-09-27", "1956-07-19", "1954-05-16", "1966-11-15", "1988-12-27", "1973-12-17", "1962-10-11", "1975-02-13", "1954-02-25", "1960-12-27", "1955-10-07", "1943-06-29", "1948-05-16", "1952-10-29", "1973-07-19", "1981-04-12", "1982-05-07", "1984-03-03", "1979-11-20", "1945-01-24", "1950-09-06", "1956-04-12", "1976-04-24", "1961-04-17", "1954-06-14", "1953-08-18", "1982-07-25", "1974-11-25", "1984-09-19", "1967-09-04", "1976-08-05", "1982-12-01", "1958-11-22", "1975-03-08", "1943-01-18", "1970-02-03", "1972-01-31", "1963-11-07", "1947-09-01", "1964-11-08", "1958-03-15", "1948-02-19", "1955-07-03", "1970-02-04", "1964-02-18", "1960-12-02", "1962-08-04", "1971-05-01", "1986-08-01", "1957-01-25", "1960-10-13", "1936-09-05", "1973-03-08", "1952-07-29", "1961-12-04", "1978-11-03", "1960-04-22", "1961-08-24", "1959-10-06", "1987-08-25", "1956-12-05", "1966-07-05", "1968-04-17", "1983-09-12", "1976-06-09", "1973-04-29", "1967-06-05", "1939-06-14", "1971-11-04", "1964-02-18", "1969-01-31", "1976-12-07", "1977-08-19", "1950-01-12", "1966-11-28", "1965-09-21", "1970-08-04", "1972-01-30", "1976-09-30", "1935-12-03", "1954-11-10", "1954-10-02", "1943-02-10", "1964-02-17", "1957-03-17", "1957-02-08", "1946-06-17", "1962-11-09", "1952-09-06", "1965-10-23", "1956-04-30", "1948-05-10", "1966-03-01", "1980-10-04", "1976-09-13", "1958-08-11", "1974-01-01", "1982-07-12", "1963-03-16", "1944-04-05", "1949-05-28", "1978-02-27", "1950-03-24", "1973-07-19", "1956-09-05", "1966-10-08", "1968-07-05", "1960-07-02", "1984-06-27", "1954-05-24", "1964-04-22",
               "1965-06-15", "1948-07-22", "1956-04-18", "1954-10-18", "1948-09-23", "1966-11-07", "1946-07-16", "1958-11-14", "1978-10-28", "1960-08-01", "1940-02-21", "1969-03-29", "1966-07-15", "1952-12-23", "1947-12-21", "1955-08-11", "1963-12-22", "1941-03-08", "1937-07-05", "1960-01-06", "1952-05-07", "1972-06-07", "1975-08-15", "1955-03-31", "1965-09-23", "1966-07-30", "1946-02-19", "1951-02-23", "1952-08-15", "1960-08-09", "1971-01-13", "1980-07-10", "1944-09-25", "1974-12-05", "1960-06-01", "1965-01-26", "1962-01-14", "1956-07-10", "1954-07-12", "1961-10-10", "1959-11-20", "1975-10-22", "1947-03-28", "1969-05-22", "1951-06-18", "1959-07-28", "1953-09-25", "1975-10-01", "1964-02-10", "1948-10-24", "1950-11-04", "1961-05-08", "1961-05-08", "1961-05-08", "1971-06-05", "1951-04-18", "1957-04-29", "1978-10-24", "1971-01-18", "1977-07-26", "1963-03-05", "1978-09-16", "1947-06-13", "1936-12-04", "1949-02-14", "1984-05-13", "1955-07-10", "1958-12-13", "1953-06-20", "1937-06-13", "1973-10-01", "1946-01-24", "1989-10-13", "1962-12-09", "1981-10-04", "1970-02-21", "1951-10-30", "1954-05-14", "1969-10-01", "1980-06-04", "1937-01-25", "1958-12-17", "1940-03-26", "1956-11-14", "1953-05-01", "1962-05-27", "1958-06-17", "1944-06-29", "1969-01-20", "1955-04-02", "1966-05-13", "1964-08-14", "1974-01-03", "1947-12-18", "1974-02-03", "1940-08-17", "1958-10-17", "1947-12-29", "1962-12-13", "1965-10-20", "1971-11-18", "1983-04-17", "1965-02-15", "1957-08-04", "1973-09-13", "1970-03-01", "1976-07-27", "1945-07-21", "1937-12-31", "1958-07-16", "1953-12-04", "1965-02-23", "1986-11-28", "1961-12-10", "1972-02-16", "1972-08-07", "1941-06-12", "1972-08-25", "1946-01-31", "1946-11-23", "1952-09-02", "1973-07-16", "1955-01-19", "1981-01-30", "1969-01-28", "1962-05-22", "1965-10-06", "1959-08-30", "1944-05-26", "1960-06-22", "1961-08-20", "1951-10-19", "1968-08-23", "1962-03-03", "1945-06-27", "1969-12-10", "1947-04-30", "1943-06-14", "1943-10-24", "1965-01-01", "1941-02-14", "1954-10-24", "1972-01-19", "1958-02-21", "1950-09-08", "1951-01-26", "1976-07-10", "1970-12-19", "1965-06-15", "1953-03-04", "1980-06-16", "1964-01-23", "1978-02-25", "1979-08-07", "1966-07-16", "1950-05-14", "1970-03-08", "1966-05-10", "1984-07-02", "1981-03-30", "1978-05-19", "1983-06-24", "1960-07-15", "1965-03-24", "1962-08-31", "1980-11-16", "1960-12-10", "1972-08-01", "1959-07-27", "1948-01-28", "1951-01-24", "1958-07-15", "1957-12-30", "1984-04-30", "1956-11-09", "1950-05-23", "1976-07-24", "1949-06-18", "1965-04-04", "1984-11-15", "1973-10-27", "1955-09-21", "1960-01-11", "1986-10-04", "1953-04-23", "1953-02-23", "1953-02-23", "1961-03-07", "1971-01-03", "1963-02-13", "1953-03-28", "1949-08-13", "1962-09-13", "1951-04-12", "1957-01-10", "1969-05-20", "1963-08-17", "1974-01-31", "1966-09-27", "1938-08-15", "1976-09-18", "1945-02-06", "1953-07-02", "1949-04-27", "1947-05-02", "1958-06-17", "1967-11-18", "1968-05-27", "1957-06-07", "1949-09-13", "1947-07-31", "1942-11-05", "1959-02-03", "1957-02-18", "1970-02-11", "1953-04-08", "1947-11-04", "1955-04-13", "1933-06-09", "1980-01-30")

#116th House Members' Party

party_116 <- c("R", "D", "R", "D", "R", "D", "R", "I", "R", "R", "R", "D", "R", "R", "R", "R", "R", "R", "D", "D", "D", "D", "R", "D", "R", "R", "R", "R", "D", "D", "D", "D", "R", "D", "R", "D", "R", "R", "D", "D", "R", "R", "R", "R", "R", "R", "D", "D", "R", "R", "D", "D", "D", "R", "R", "D", "D", "D", "D", "D", "R", "R", "D", "D", "D", "D", "D", "D", "D", "R", "R", "D", "D", "R", "R", "R", "R", "R", "D", "R", "D", "D", "D", "D", "D", "D", "R", "R", "D", "D", "D", "D", "D", "R", "D", "R", "D", "R", "D", "D", "D", "D", "D", "D", "D", "D", "D", "R", "D", "R", "D", "D", "D", "R", "R", "R", "R", "D", "D", "D", "D", "R", "D", "R", "D", "R", "R", "D", "R", "R", "D", "R", "D", "D", "R", "D", "R", "R", "D", "D", "D", "D", "R", "R", "R", "R", "D", "D", "R", "D", "R", "R", "R", "D", "R", "R", "R", "R", "D", "R", "R", "D", "R", "R", "R", "D", "R", "D", "D", "R", "R", "D", "D", "D", "R", "R", "R", "R", "D", "D", "R", "D", "R", "R", "D", "D", "D", "D", "R", "D", "R", "R", "R", "D", "R", "D", "D", "R", "R", "D", "R", "D", "R", "R", "R", "R", "D", "R", "D", "R", "D", "R", "R", "D", "D", "D", "D", "D", "D", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "R", "D", "D", "D", "R", "D", "D", "D", "D", "R", "D", "D", "D", "D", "D", "D", "D", "R", "R", "D", "D", "R", "R", "D", "D", "D", "D", "D", "D", "R", "R", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "R", "D", "D", "R", "D", "R", "I", "R", "R", "R", "D", "D", "D", "D", "R", "R", "D", "D", "D", "D", "D", "R", "D", "R", "D", "R", "D", "D", "R", "D", "R", "D", "R", "D", "D", "D", "D", "D", "R", "D", "R", "D", "D", "D", "D", "D", "D", "D", "R", "D", "D", "D", "R", "D", "R", "R", "R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "R", "D", "D", "R", "R", "D", "D", "D", "D", "R", "D", "D", "D", "D", "D", "R", "D", "D", "D", "D", "D", "D", "R", "D", "R", "D", "R", "D", "D", "D", "D", "D", "R", "R", "D", "D", "R", "D", "R", "R", "R", "D", "D", "R", "D", "D", "R", "R", "R", "R", "D", "R", "R", "D", "D", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "D", "D", "D", "D", "D", "R", "D", "R", "D", "R", "D", "D", "D", "D", "D", "R", "R", "R", "R", "R", "R", "D", "D", "R", "D", "R", "R", "D", "R", "R", "D", "D", "R", "R", "D", "R", "R", "R", "R", "D", "R", "R", "R")


#117th House Members' Gender

gender_117 <- c("F", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "F", "F", "M", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "F", "F", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "F", "F", "F", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "F", "M", "M", "F", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "F", "M", "F", "M", "F", "F", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "F", "F", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "F", "M", "F", "M", "F", "M", "F", "M", "M", "F", "F", "M", "M", "F", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "F", "F", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "M", "F", "M", "F", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "F", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "F", "F", "F", "M", "M", "F", "F", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "F", "M", "F", "M", "F", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "F", "F", "M", "F", "M", "F", "F", "F", "M", "M", "M", "M", "M", "F", "F", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M")


#117th House Members' Birth Date

birth_117 <- c("1946-05-27", "1965-07-22", "1979-06-19", "1951-11-07", "1983-04-15", "1958-06-12", "1976-10-08", "1972-03-09", "1988-01-29", "1965-04-20", "1948-03-23", "1963-08-16", "1945-06-04", "1962-01-16", "1979-07-16", "1973-07-24", "1976-09-15", "1953-10-03", "1950-03-12", "1952-01-12", "1965-03-02", "1947-02-02", "1950-06-20", "1973-11-11", "1958-11-07", "1963-02-08", "1964-07-01", "1947-02-04", "1948-08-16", "1962-02-10", "1986-12-15", "1954-10-14", "1960-12-30", "1970-06-03", "1976-04-01", "1977-02-06", "1955-04-11", "1954-04-29", "1975-06-24", "1961-11-21", "1952-08-28", "1951-05-08", "1959-02-16", "1962-05-31", "1971-10-21", "1964-08-25", "1950-12-23", "1976-07-21", "1961-10-17", "1947-04-27", "1953-06-08", "1988-02-16", "1964-11-18", "1963-03-31", "1971-03-13", "1958-06-17", "1974-10-16", "1963-10-26", "1957-09-06", "1941-11-06", "1961-05-01", "1952-09-27", "1971-11-23", "1966-08-20", "1974-09-16", "1995-08-01", "1953-01-22", "1966-07-28", "1979-01-25", "1953-07-07", "1961-07-15", "1963-07-17", "1964-11-21", "1944-10-26", "1972-02-29", "1975-05-13", "1940-07-21", "1963-11-22", "1949-05-24", "1949-04-28", "1972-08-19", "1950-03-30", "1954-06-19", "1958-01-24", "1952-04-13", "1953-04-06", "1972-02-14", "1966-01-22", "1984-03-14", "1956-07-24", "1979-03-15", "1955-09-19", "1960-05-10", "1980-05-22", "1970-03-01", "1970-01-05", "1941-09-06", "1959-06-06", "1947-05-27", "1957-07-29", "1943-03-02", "1962-02-17", "1977-01-19", "1957-03-12", "1952-03-31", "1964-02-21", "1966-05-07", "1961-09-25", "1953-11-23", "1946-10-06", "1972-10-28", "1953-08-05", "1966-01-07", "1953-02-16", "1970-01-24", "1961-03-03", "1969-09-15", "1942-12-13", "1954-09-27", "1956-07-19", "1954-05-16", "1967-12-19", "1969-01-14", "1966-11-15", "1959-07-01", "1965-11-03", "1963-11-16", "1973-12-17", "1962-10-11", "1975-02-13", "1960-12-27", "1955-10-07", "1943-06-29", "1948-05-16", "1964-08-23", "1952-10-29", "1973-07-19", "1982-05-07", "1984-03-03", "1979-11-20", "1945-01-24", "1984-09-27", "1976-04-24", "1956-04-12", "1950-09-06", "1954-06-14", "1954-01-17", "1953-08-18", "1982-07-25", "1974-11-25", "1980-10-10", "1984-09-19", "1967-09-04", "1976-08-05", "1965-09-11", "1982-12-01", "1958-11-22", "1975-03-08", "1943-01-18", "1963-11-07", "1972-01-31", "1964-11-08", "1947-09-01", "1974-05-27", "1958-03-15", "1948-02-19", "1955-07-03", "1970-02-04", "1964-02-18", "1960-12-02", "1962-08-04", "1986-08-01", "1957-01-25", "1960-01-01", "1960-10-13", "1936-09-05", "1973-03-08", "1961-12-04", "1964-03-16", "1978-11-03", "1960-04-22", "1959-10-06", "1961-08-24", "1956-12-05", "1966-07-05", "1983-06-27", "1983-09-12", "1973-04-29", "1967-06-05", "1939-06-14", "1971-11-04", "1964-02-18", "1969-01-31", "1953-11-01", "1967-05-04", "1950-01-12", "1989-02-01", "1966-11-28", "1965-09-21", "1970-08-04", "1954-10-02", "1935-12-03", "1972-01-30", "1976-09-30", "1954-11-10", "1987-05-18", "1964-02-17", "1957-02-08", "1957-03-17", "1974-03-28", "1946-06-17", "1962-11-09", "1952-09-06", "1965-10-23", "1948-05-10", "1956-04-30", "1966-03-01", "1976-09-13", "1958-08-11", "1974-01-01", "1982-07-12", "1962-10-18", "1963-03-16", "1978-02-27",
               "1950-03-24", "1973-07-19", "1956-09-05", "1966-10-08", "1968-07-05", "1960-07-02", "1984-06-27", "1954-05-24", "1964-04-22", "1965-06-15", "1948-07-22", "1956-04-18", "1988-02-17", "1954-10-18", "1948-09-23", "1946-07-16", "1966-11-07", "1958-11-14", "1981-03-16", "1978-10-28", "1960-08-01", "1969-03-29", "1947-12-21", "1955-08-11", "1963-12-22", "1941-03-08", "1960-01-06", "1952-05-07", "1975-08-15", "1955-03-31", "1977-12-04", "1965-09-23", "1980-11-11", "1966-07-30", "1946-02-19", "1976-12-17", "1956-12-03", "1971-01-13", "1980-07-10", "1944-09-25", "1960-06-01", "1965-01-26", "1962-01-14", "1966-04-07", "1956-07-10", "1954-07-12", "1961-10-10", "1959-11-20", "1975-10-22", "1947-03-28", "1969-05-22", "1951-06-18", "1953-09-25", "1988-01-10", "1975-10-01", "1964-02-10", "1948-10-24", "1959-08-27", "1950-11-04", "1955-09-06", "1961-05-08", "1971-06-05", "1980-06-22", "1951-04-18", "1966-09-26", "1957-04-29", "1978-10-24", "1969-04-16", "1977-07-26", "1963-03-05", "1978-09-16", "1947-06-13", "1936-12-04", "1949-02-14", "1984-05-13", "1968-04-07", "1955-07-10", "1964-04-13", "1958-12-13", "1953-06-20", "1937-06-13", "1973-10-01", "1946-01-24", "1970-08-18", "1989-10-13", "1981-10-04", "1951-08-02", "1970-02-21", "1951-10-30", "1954-05-14", "1969-10-01", "1980-06-04", "1937-01-25", "1958-12-17", "1940-03-26", "1956-11-14", "1953-05-01", "1962-05-27", "1958-06-17", "1978-12-28", "1969-01-20", "1955-04-02", "1966-05-13", "1964-08-14", "1974-01-03", "1947-12-18", "1974-02-03", "1940-08-17", "1958-10-17", "1947-12-29", "1962-12-13", "1971-11-18", "1983-04-17", "1965-02-15", "1957-08-04", "1973-09-13", "1958-07-16", "1937-12-31", "1965-02-23", "1960-07-07", "1963-06-20", "1972-02-16", "1972-08-07", "1941-06-12", "1972-08-25", "1946-01-31", "1946-11-23", "1952-09-02", "1973-07-16", "1955-01-19", "1961-11-01", "1981-01-30", "1969-01-28", "1962-05-22", "1965-10-06", "1959-08-30", "1944-05-26", "1960-06-22", "1961-08-20", "1951-10-19", "1968-08-23", "1962-03-03", "1945-06-27", "1947-04-30", "1969-12-10", "1955-03-22", "1965-01-01", "1954-10-24", "1972-01-19", "1950-09-08", "1951-01-26", "1976-07-10", "1980-06-16", "1970-12-19", "1965-06-15", "1953-03-04", "1964-01-23", "1978-02-25", "1979-08-07", "1978-10-06", "1950-05-14", "1979-01-31", "1970-03-08", "1966-05-10", "1955-06-21", "1984-07-02", "1981-03-30", "1978-05-19", "1983-06-24", "1960-07-15", "1965-03-24", "1962-09-25", "1962-08-31", "1980-11-16", "1960-12-10", "1972-08-01", "1961-02-04", "1951-01-24", "1948-01-28", "1959-07-27", "1957-12-30", "1984-04-30", "1950-05-23", "1976-07-24", "1949-06-18", "1965-04-04", "1988-03-12", "1973-10-27", "1955-09-21", "1960-01-11", "1986-10-04", "1953-04-23", "1977-04-14", "1953-02-23", "1970-11-16", "1961-03-07", "1971-01-03", "1963-02-13", "1953-03-28", "1962-09-13", "1951-04-12", "1963-08-17", "1974-01-31", "1966-09-27", "1938-08-15", "1945-02-06", "1953-07-02", "1949-04-27", "1947-05-02", "1958-06-17", "1967-11-18", "1968-05-27", "1957-06-07", "1949-09-13", "1978-07-30", "1947-07-31", "1942-11-05", "1959-02-03", "1957-02-18", "1953-04-08", "1947-11-04", "1933-06-09", "1980-01-30")

#117th House Members' Party

party_117 <- c("D", "R", "D", "R", "D", "R", "R", "R", "D", "D", "R", "R", "R", "R", "R", "R", "D", "D", "D", "R", "D", "R", "D", "R", "R", "R", "R", "D", "D", "D", "R", "D", "R", "D", "D", "D", "R", "R", "D", "D", "D", "R", "R", "R", "R", "R", "R", "D", "D", "D", "R", "R", "D", "D", "R", "R", "D", "D", "R", "R", "D", "D", "D", "D", "D", "R", "R", "R", "D", "D", "D", "D", "D", "D", "R", "R", "D", "R", "D", "R", "R", "D", "D", "D", "D", "D", "D", "R", "R", "D", "D", "D", "R", "D", "R", "R", "D", "D", "D", "D", "D", "D", "D", "D", "D", "R", "D", "R", "D", "D", "R", "D", "R", "R", "R", "R", "D", "D", "D", "R", "D", "R", "R", "R", "D", "R", "R", "R", "R", "D", "R", "D", "R", "D", "R", "D", "R", "R", "R", "D", "D", "R", "R", "D", "D", "R", "R", "R", "D", "D", "R", "R", "D", "R", "R", "R", "R", "D", "R", "R", "R", "R", "D", "R", "R", "D", "R", "R", "R", "D", "R", "D", "R", "R", "R", "D", "D", "R", "R", "R", "R", "D", "R", "R", "D", "R", "R", "D", "D", "D", "R", "D", "R", "R", "R", "D", "D", "R", "D", "D", "D", "D", "R", "R", "R", "D", "R", "R", "R", "D", "D", "R", "D", "R", "R", "D", "R", "D", "D", "D", "D", "R", "D", "R", "D", "D", "D", "R", "R", "R", "D", "R", "D", "D", "D", "R", "R", "D", "D", "D", "D", "R", "R", "D", "D", "D", "D", "R", "R", "D", "R", "R", "D", "D", "R", "D", "R", "D", "D", "R", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "D", "R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "D", "R", "D", "D", "D", "R", "R", "D", "D", "D", "D", "D", "R", "R", "D", "D", "R", "D", "R", "D", "R", "D", "D", "R", "R", "D", "R", "D", "D", "D", "D", "D", "R", "D", "R", "D", "R", "D", "D", "D", "D", "D", "R", "D", "D", "D", "R", "D", "R", "R", "D", "R", "D", "R", "R", "R", "R", "D", "R", "R", "D", "D", "D", "D", "R", "D", "D", "R", "D", "D", "D", "R", "D", "D", "D", "D", "D", "D", "R", "D", "D", "R", "R", "D", "D", "D", "R", "D", "D", "R", "R", "D", "R", "R", "D", "D", "R", "D", "D", "D", "R", "R", "R", "R", "R", "D", "R", "R", "D", "D", "D", "D", "R", "R", "D", "D", "R", "R", "R", "D", "D", "D", "D", "D", "D", "D", "R", "D", "R", "R", "R", "R", "D", "D", "D", "D", "R", "R", "R", "R", "D", "D", "D", "R", "R", "D", "R", "R", "D", "D", "R", "D", "R", "D", "R", "R", "R", "D", "R", "R")






#Class: Week 02
#Course: Big Data and Social Analysis
#Semester: Spring 2023
#Lesson: R, Vector, and Object
#Instructor: Chung-pei Pien
#Organization: ICI, NCCU

### Student Information --------

#Chinese Name: 嘉博
#English First Name: Jasper
#UID: 111926019
#E-mail: jasper.hewitt@me.com

### Questions --------

#Please answer the following questions. Remember, No Comments, No Points!!!!!!!

#____________________________________________________________________________________

#Question 1: (3 points)
#From 115th-117th terms of US house, which term has the largest number of lawmakers?

# Load ggplot2, I would like to plot the result for more clearity. 
library(ggplot2)

length(gender_115) #456 MOST LAWMAKERS
length(gender_116) #451
length(gender_117) #447

# the answer is already there, but I'm just exploring some of the possibilities in R. 
# in the following lines I create a dataframe to plot the results. 

total_lawmaker_data <- data.frame(
  term=c("115th term","116th term","117th term") ,  
  total_lawmakers=c(length(gender_115), length(gender_116), length(gender_117))
)

# create a barplot with with ggplot. 
# In order to display the discrepancy between the bars more clearly, I zoomed in on on the Y range 400 to 500 with ylim.

ggplot(total_lawmaker_data, aes(x=term, y=total_lawmakers)) + 
  geom_bar(stat = "identity") +
  coord_cartesian(ylim=c(400, 500))

#answer: the 115th term has the most lawmakers

#____________________________________________________________________________________

#Question 2: (9 points)
#From 115th-117th terms of US house, which term has worse gender inequality performance?

# calculating the ratio per term, number of females divided by the total number of lawmakers and assigning it to different objects. 
ratio_115 <- length(gender_115[gender_115=='F'])/length(gender_115)  #0.2039 WORST PERFORMANCE FOR GENDER INEQUALITY
ratio_116 <- length(gender_116[gender_116=='F'])/length(gender_116)  #0.2350 
ratio_117 <- length(gender_117[gender_117=='F'])/length(gender_117)  #0.2841

# creating another dataframe to plot the results
ratio_data <- data.frame(
  term=c("115th term","116th term","117th term") ,  
  ratio_per_term=c(ratio_115, ratio_116, ratio_117)
)

# plotting the results, this time with an y minimum of 1, because all the ratios are between 0 and 1
ggplot(ratio_data, aes(x=term, y=ratio_per_term)) + 
  geom_bar(stat = "identity") +
  coord_cartesian(ylim=c(0, 1))

#answer: the 115th term has the worst performance in terms of gender inequality

#____________________________________________________________________________________

#Question 3: (9 points)
#From 115th-117th terms of US house, which term's age is oldest?

#I'm using the lubricate package to calculate the age.
require(lubridate) 

# the lubricate package obviously requires two variables to calculate age.
# I used https://www.senate.gov/legislative/DatesofSessionsofCongress.htm to see what the starting
# dates of the different terms were in order to properly calculate the ages of its members. 
# using Sys.Date() will lead to an incorrect answer because 115th will just be the oldest and 117th the youngest.

#lubricate can calculate the time between two dates in seconds. You then have to divide it by years to get the final output
age_115<-trunc((birth_115 %--% "2017-01-03") / years(1)) #explain what it does!
mean_age_115 <- mean(age_115) #57.51

age_116<-trunc((birth_116 %--% "2019-01-03") / years(1)) #explain what it does!
mean_age_116 <-mean(age_116) #56.91

age_117<-trunc((birth_117 %--% "2021-01-03") / years(1)) #explain what it does!
mean_age_117 <- mean(age_117) #57.53 #OLDEST AGE

# creating another dataframe to plot the results
mean_age_data <- data.frame(
  term=c("115th term","116th term","117th term") ,  
  mean_age_per_term=c(mean_age_115, mean_age_116, mean_age_117)
)

#plotting the results, this time with an ylim between 55 and 60. 
ggplot(mean_age_data, aes(x=term, y=mean_age_per_term)) + 
  geom_bar(stat = "identity") +
  coord_cartesian(ylim=c(55, 60))

#answer: 117th term has the oldest average age. 



#____________________________________________________________________________________
#Question 4: (9 points)
#Please tell me the control party of 115th-117th term of US House

#which party dominate the house 
dem_115<-length(party_115[party_115=='D']) #204
rep_115<-length(party_115[party_115=='R']) #252 republicans dominate

dem_116<-length(party_116[party_116=='D']) #241 democrats dominate
rep_116<-length(party_116[party_116=='R']) #208

dem_117<-length(party_117[party_117=='D']) #230 democrats dominate
rep_117<-length(party_117[party_117=='R']) #217


# creating another dataframe to plot the results
dominance_data <- data.frame(
  term_party=c("115th democrats", "115th republicans", "116th democrats", "116th republicans", "117th democrats", "117th republicans"),  
  seats_per_party=c(dem_115, rep_115, dem_116, rep_116, dem_117, rep_117)
)

#plotting the results, this time with an ylim between 200 and 300. 
ggplot(dominance_data, aes(x=term_party, y=seats_per_party)) + 
  geom_bar(stat = "identity") +
  coord_cartesian(ylim=c(200, 300))

#answer, 115th= republicans, 116th and 117th=democrats

#week 2 and homework _____________________________________________________________________________________________________
#basic skills 

#assignment, how many male how many female in the class

students <- c('m', 'f', 'f', 'm', 'm', 'm', 'f', 'f', 'f', 'f')

age <- c(45, 60, 22, 61, 34, 59, 64, 54, 29, 31)

id <- c(111, 222, 333, 444, 555, 666, 777, 888, 999, 100 )

nccu <- 'universty'

#vector is a set of numbers (numeric) or characters created by c()function
nccu <- c('Taipei', 'ici', 'zoo', 'beautiful')
z <- c(1, 5, 10, 15, 27)

#make sure that your strings are strings '' and your numbers are numbers

#select specific part in the list
#unlike python, it doesn't start at 0, but at 1
nccu[1]
nccu[2]
nccu[3]

nccu[1:3] #one to three 
nccu[c(1, 3:4)]
nccu
nccu[-2]
nccu

#now what if you want to change the first one 
nccu[1] <- c('home') 
nccu      #now it's changed 

nccu[5] <- c('international') #add another one


#saved code by teacher


#Vector

#Vector Example 1

#assignment, how many male how many female in the clas, see the spiekbrief

student <- c("m", "f", "f", "m", "m", "m", "f", "f", "f", "f")

#this code counts the sum of the number of 'm' and 'f' in the vector 'student'
sum(student == 'm') #number of male students=4
sum(student == 'f') #number of female students=6

#other ways to do it
student[student=="m"]
length(student[student=="m"])



#Vector Example 2

age <- c(45, 60, 22, 61, 34, 59, 64, 54, 29, 31)

#1. how many workers in the company 
length(age) #output: 10

#2. how many workers larger than 60
length(age[age>=60]) #output: 3

#3. how many workers' age are smaller than 30
length(age[age<=30]) #output: 2

#ratio of male students (male students divided by total)
length(student[student=='m'])/length(student)
length(student[student=='f'])/length(student)


#Vector Example 3

id <- c("111111222", "110222111", "110222333", "109333444", "111555666")
#how many students are undergrads: starts with 111
nchar(id)



#what about freshman students


#Vector Example 4

uid <- c("111111222", "110222111", "110222333", "109333444", "111555666")
#how many undergraduate and freshman students are in theis class 
undergrad<-substr(id, 5, 5)
undergrad
length(undergrad[undergrad=='0']) #output: 0

#freshman students
year<-substr(id, 1, 3) #cut the first three letters from id and assign them to a new variable 'year'
length(year[year=='111']) #output, 2, If first year masterstudents are considered freshman

length(year[year=='111' & undergrad == 0]) #output:0

#Vector Example 5

birth <- c("2002", "1999", "2001", "1998", "2000")
birth <- as.numeric(birth) #convert to to numeric
age <- 2023 - birth #year - birth = age
median(age) #median age 




#Vector Example 6
#ok try to figure this one out. average age of the players

install.packages(eeptools)?
  
  birthdate <- c("1983-11-01", "1995-01-19", "2001-06-23", "1987-12-09", "1999-10-21", "1999-03-31")
year <- birthdate<-substr(birthdate, 1, 4)
year <- as.numeric(birth)
rough_age <- 2023-year








####115th US Congress Data

gender <- c("M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "F", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "F", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "F", "F", "F", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "F", "M", "M", "F", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "F", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "F", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "F", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "F", "F", "M", "M", "M", "M", "M", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "F", "F", "M", "F", "F", "F", "M", "M", "M", "M", "M", "F", "M", "F", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M", "M")

birth <- c("1954-09-16", "1946-05-27", "1965-07-22", "1979-06-19", "1951-11-07", "1980-04-18", "1958-06-12", "1972-03-09", "1948-03-23", "1963-08-16", "1962-01-16", "1979-07-16", "1956-01-28", "1973-07-24", "1976-09-15", "1949-09-15", "1953-10-03", "1950-03-12", "1958-01-26", "1965-03-02", "1947-02-02", "1950-06-20", "1958-11-07", "1963-02-08", "1967-03-18", "1951-07-13", "1947-02-04", "1951-01-16", "1952-06-06", "1955-04-26", "1948-08-16", "1962-02-10", "1954-10-14", "1933-05-31", "1960-12-30", "1977-02-06", "1955-04-11", "1945-04-07", "1964-07-27", "1975-06-15", "1960-08-25", "1954-04-29", "1961-11-21", "1952-08-28", "1951-05-08", "1959-02-16", "1962-05-31", "1971-10-21", "1950-12-23", "1961-10-17", "1947-04-27", "1955-02-16", "1953-06-08", "1952-01-09", "1964-11-18", "1963-03-31", "1974-10-16", "1941-11-06", "1957-09-06", "1961-05-01", "1966-08-20", "1974-09-16", "1953-01-22", "1967-03-26", "1966-07-28", "1953-07-07", "1961-07-15", "1963-07-17", "1964-11-21", "1956-07-27", "1944-10-26", "1975-05-13", "1940-07-21", "1955-03-19", "1949-05-24", "1949-04-28", "1950-05-20", "1966-08-16", "1972-08-19", "1959-06-30", "1948-06-11", "1950-03-30", "1929-05-16", "1943-03-03", "1954-06-19", "1958-01-24", "1952-04-13", "1976-09-07", "1953-04-06", "1961-01-21", "1966-01-22", "1956-07-24", "1962-03-16", "1955-09-19", "1956-08-24", "1951-01-18", "1980-03-01", "1960-05-10", "1970-03-01", "1941-09-06", "1970-01-05", "1944-04-13", "1947-05-27", "1957-07-29", "1963-04-16", "1943-03-02", "1962-02-17", "1957-03-12", "1967-07-29", "1960-05-24", "1978-09-14", "1952-03-31", "1964-02-21", "1966-05-07", "1961-09-25", "1953-11-23", "1946-10-06", "1956-11-06", "1953-08-05", "1971-10-03", "1966-01-07", "1947-07-21", "1953-02-16", "1963-08-04", "1961-03-03", "1947-02-18", "1942-12-13", "1954-09-27", "1956-07-19", "1959-08-25", "1954-05-16", "1961-12-12", "1952-08-25", "1966-11-15", "1973-12-17", "1962-10-11", "1954-02-25", "1960-12-27", "1955-10-07", "1943-06-29", "1948-05-16", "1957-06-19", "1946-04-29", "1952-10-29", "1981-04-12", "1982-05-07", "1984-03-03", "1979-11-20", "1945-01-24", "1972-03-27", "1961-04-17", "1954-06-14", "1953-08-18", "1974-11-25", "1967-09-04", "1976-08-05", "1952-09-22", "1958-11-22", "1975-03-08", "1964-08-22", "1943-01-18", "1972-01-31", "1963-11-07", "1970-02-03", "1947-10-17", "1947-09-01", "1958-03-15", "1948-02-19", "1955-07-03", "1964-02-18", "1953-12-10", "1951-05-04", "1962-04-18", "1956-06-01", "1957-01-25", "1960-10-13", "1936-09-05", "1952-07-29", "1957-05-29", "1961-12-04", "1978-11-03", "1960-04-22", "1959-10-06", "1961-08-24", "1956-12-05", "1966-07-05", "1968-04-17", "1983-09-12", "1939-06-14", "1971-11-04", "1964-02-18", "1969-01-31", "1966-03-01", "1976-12-07", "1977-08-19", "1953-11-01", "1950-01-12", "1965-09-21", "1970-08-04", "1960-09-12", "1963-06-10", "1954-10-02", "1954-11-10", "1935-12-03", "1930-10-11", "1972-01-30", "1959-10-24", "1943-02-10", "1964-02-17", "1957-03-17", "1946-06-17", "1962-11-09", "1952-09-06", "1966-03-01", "1956-04-30", "1948-05-10", "1980-10-04", "1976-09-13", "1980-04-25", "1958-08-11", "1974-01-01", "1963-03-16", "1944-04-05", "1949-05-28", "1978-02-27", "1966-12-17", "1973-07-19", "1956-09-05", "1966-10-08", "1967-12-08", "1968-07-05", "1960-07-02", "1984-06-27", "1954-05-24", "1952-06-25", "1964-04-22", "1965-06-15", "1948-07-22", "1956-04-18", "1954-10-18", "1948-09-23", "1946-07-16", "1958-11-14", "1931-09-06", "1955-09-23", "1940-02-21", "1969-03-29", "1966-07-15",
           "1946-05-12", "1952-12-23", "1947-12-21", "1955-08-11", "1963-12-22", "1975-12-06", "1941-03-08", "1937-07-05", "1960-01-06", "1952-05-07", "1972-06-07", "1959-10-24", "1955-03-31", "1960-10-16", "1946-02-19", "1966-07-30", "1951-02-23", "1952-08-15", "1960-08-09", "1971-01-13", "1980-07-10", "1944-09-25", "1965-01-26", "1962-01-14", "1956-07-10", "1954-07-12", "1961-10-10", "1959-11-20", "1975-10-22", "1947-03-28", "1969-05-22", "1951-06-18", "1966-03-22", "1959-07-28", "1955-10-20", "1953-09-25", "1975-10-01", "1969-02-27", "1961-05-08", "1961-05-08", "1971-06-05", "1951-04-18", "1957-04-29", "1978-10-24", "1977-07-26", "1967-07-21", "1978-09-16", "1952-09-11", "1947-06-13", "1936-12-04", "1949-02-14", "1955-07-10", "1971-11-30", "1943-12-17", "1958-12-13", "1953-06-20", "1937-06-13", "1973-10-01", "1946-01-24", "1972-09-26", "1962-12-09", "1970-02-21", "1951-10-30", "1954-05-14", "1969-10-01", "1937-01-25", "1965-05-14", "1958-12-17", "1947-08-24", "1940-03-26", "1953-05-01", "1962-05-27", "1958-06-17", "1944-06-29", "1955-04-02", "1948-08-15", "1966-05-13", "1964-08-14", "1948-09-10", "1953-11-01", "1975-05-12", "1963-12-30", "1947-12-18", "1940-08-17", "1954-10-08", "1958-10-17", "1947-12-29", "1962-12-13", "1965-10-20", "1971-11-18", "1950-08-29", "1958-12-03", "1965-02-15", "1957-08-04", "1973-09-13", "1976-07-27", "1945-07-21", "1958-07-16", "1937-12-31", "1947-06-21", "1970-02-09", "1953-12-04", "1970-11-21", "1952-07-15", "1957-08-02", "1961-09-13", "1959-10-18", "1962-04-25", "1972-02-16", "1941-06-12", "1951-10-12", "1972-08-25", "1946-01-31", "1946-11-23", "1963-05-25", "1952-09-02", "1970-01-29", "1973-07-16", "1955-01-19", "1969-01-28", "1960-05-28", "1962-05-22", "1965-10-06", "1959-08-30", "1944-05-26", "1960-06-22", "1961-08-20", "1951-10-19", "1962-03-03", "1947-04-30", "1945-06-27", "1969-12-10", "1943-06-14", "1943-10-24", "1955-03-22", "1965-01-01", "1952-12-02", "1954-10-24", "1958-02-21", "1961-01-10", "1950-09-08", "1976-07-12", "1951-01-26", "1929-08-14", "1953-03-04", "1947-11-19", "1980-06-16", "1965-06-15", "1970-12-19", "1964-01-23", "1978-02-25", "1950-05-14", "1984-07-02", "1960-07-15", "1965-03-24", "1962-08-31", "1980-11-16", "1960-12-10", "1979-06-27", "1961-02-04", "1948-01-28", "1951-01-24", "1959-07-27", "1958-07-15", "1962-10-21", "1956-11-09", "1950-05-23", "1949-06-18", "1965-04-04", "1960-10-16", "1946-04-26", "1960-01-11", "1953-04-23", "1977-04-14", "1961-03-07", "1971-01-03", "1963-02-13", "1953-03-28", "1949-08-13", "1962-09-13", "1951-04-12", "1957-01-10", "1969-05-20", "1963-08-17", "1962-05-14", "1964-04-06", "1966-09-27", "1938-08-15", "1945-02-06", "1953-07-02", "1949-04-27", "1947-05-02", "1958-06-17", "1967-11-18", "1957-06-07", "1949-09-13", "1942-11-05", "1947-07-31", "1959-02-03", "1957-02-18", "1970-02-11", "1947-11-04", "1976-01-08", "1955-04-13", "1933-06-09", "1968-05-11", "1980-01-30", "1961-11-01")

party <- c("R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "R", "R", "R", "R", "D", "R", "D", "D", "D", "D", "R", "D", "R", "R", "R", "R", "D", "R", "R", "R", "D", "D", "D", "D", "R", "D", "R", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "R", "R", "D", "D", "R", "R", "D", "D", "D", "D", "R", "R", "D", "D", "D", "R", "R", "R", "D", "D", "D", "D", "D", "D", "R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "D", "D", "R", "D", "D", "D", "R", "D", "R", "R", "D", "D", "D", "R", "D", "R", "R", "R", "D", "R", "D", "D", "D", "D", "D", "D", "D", "R", "R", "R", "D", "R", "D", "R", "D", "D", "R", "D", "R", "R", "R", "R", "D", "R", "D", "D", "D", "R", "D", "D", "R", "R", "R", "R", "R", "R", "R", "D", "R", "D", "R", "R", "D", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "D", "R", "R", "R", "R", "R", "D", "D", "R", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "R", "D", "R", "R", "D", "R", "R", "D", "R", "D", "R", "R", "R", "R", "R", "D", "D", "D", "R", "R", "D", "R", "D", "R", "R", "D", "R", "R", "R", "D", "R", "D", "R", "D", "R", "D", "D", "D", "D", "D", "D", "R", "R", "R", "R", "D", "D", "R", "R", "R", "R", "D", "R", "R", "D", "D", "D", "R", "D", "D", "D", "R", "D", "R", "D", "D", "D", "R", "D", "D", "R", "R", "R", "D", "D", "R", "R", "D", "D", "D", "R", "D", "D", "R", "R", "R", "R", "R", "D", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "R", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "D", "R", "R", "D", "R", "D", "D", "D", "R", "R", "D", "D", "R", "D", "R", "D", "D", "R", "R", "D", "R", "D", "D", "R", "D", "R", "D", "D", "R", "D", "D", "D", "R", "D", "D", "R", "R", "D", "R", "R", "D", "R", "D", "R", "D", "R", "R", "R", "R", "D", "R", "D", "R", "R", "R", "R", "R", "R", "R", "R", "R", "D", "R", "R", "R", "R", "D", "R", "D", "D", "D", "R", "R", "R", "D", "D", "D", "R", "D", "R", "D", "D", "D", "D", "D", "R", "D", "D", "R", "R", "D", "R", "D", "D", "D", "R", "R", "R", "D", "D", "D", "R", "R", "R", "D", "R", "R", "D", "D", "R", "R", "R", "D", "D", "D", "R", "R", "D", "D", "R", "R", "R", "R", "D", "D", "D", "R", "D", "R", "R", "R", "D", "D", "D", "D", "D", "R", "R", "R", "R", "R", "R", "D", "D", "D", "D", "R", "R", "D", "R", "R", "D", "R", "D", "R", "R", "R", "R", "D", "R", "R", "R", "R", "R", "R")

install.packages('lubricate')

#length(gender[gender=='M'])/length(gender)

#female ratio of the lawmakers
length(gender[gender=='F'])#/length(gender) #0.2039474
length(gender)
#the mean of the lawmakers' age
x_date<- as.Date("2023-02-23")

require(lubridate) #use lubricate package, also check out 'zoom', another package.

age<-trunc((birth %--% x_date) / years(1)) #explain what it does!
mean(age) #63.66447

#which party dominate the house 
dem<-length(party[party=='D'])
rep<-length(party[party=='R'])
dem
rep #republicans dominate the house 

#HOMEWORK evaluate changes in gender inequaltiy, age, party's control from 115th to 117th US congress



#question 5 is probably because of the independents!!!
#use unique and try to fix it. say that the data he provided doesn't include the independents. 

#week 3 and homework_____________________________________________________________________________________________________

donate_df <- data.frame(code=c("001", "002", "003", "004", "005"),
                        gender = c("M", "F", "F", "M", "F"),
                        value= c(500, 300, 1000, 500, 300))

str(donate_df)

#get all data in a given column
donate_df$value #shows all data in the value column 
donate_df$gender #for gender

#create vector object and get the data from the table
money <- donate_df$value
money #now money is the same as donate_df$value

#you can also use simple commands to do some stuff with the dataframe
sum(donate_df$value) #shows the sum of all the cells in the column 
mean(donate_df$value) #shows the mean
median(donate_df$value) #median

#little assignment!
#compare male and female donation, so only calculate the sum for value where gender=="M" or "F"
#sum
sum(donate_df$value[donate_df$gender=="M"])
mean(donate_df$value[donate_df$gender=="F"])
#mean
mean(donate_df$value[donate_df$gender=="F"])
mean(donate_df$value[donate_df$gender=="M"])

#import data
#1. create a new folder 

#2. create a new R file and put it in the same folder (see map in 'code')

#3. set working directory where the csv file is

setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/house")

#get the file
house_115 <- read.csv("house_115.csv")

library(readxl)

setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets/")
trump <- read_xlsx('trumpscore.xlsx')

#basic cleaning of dataframe
#create a variable called USD: exchange value(TW) to USD, divide by 28 for now
#donate_df$usd is an easy way to name a new column
donate_df$usd <- donate_df$value/28 #new column usd <- $value/28 (value column)

#create a variable called sex: Female is 1 and male is 0 
#just create a new column where everything is 0, and then if they are female you change it to 1
donate_df$sex <- 0
donate_df$sex[donate_df$gender=="F"] <-1

#create a varible called donate: value  >= 500 is 2, and value smaller than 500 is 1
#一樣的道理, first create a new column with the default value of 1
#then change all the other ones that meet a certain criteria (in this case higher than 500).
#this is defo a lot faster than an if statement
donate_df$donate<-1
donate_df$donate[donate_df$value>=500]<-2

#create a new dataframe called male_donate, so take all the rows where gender =="M"
male_donate <- donate_df[donate_df$gender=="M",]

#now create one for female, so take all the rows where gender =="F"
#this time we only want it to have 2-3 columns, you can use different ways to select the columns you want
# -1 just takes away the first column, etc. 
f_donate <- donate_df[donate_df$gender=="F", c(1, 2, 3)]

#now let's get back to house_115
house_115
#check the observations and variables
nrow(house_115) #show how many rows

colnames(house_115) #show all the column names

#if you don't like the name, here is one way that you can change it
#but there are probably better ways to do it. 

colnames(house_115)[7]<-c('birth') #and now it's changed 
house_115$birth

#you can also change multiple column names at the same time, THIS DIDN"T WORK ERROR
colnames(house_115)[7, 10:11]<- c('birth', 'twitter', 'facebook')

#sometimes you want to make a new dataframe that is a subselection of the main dataframe
house_a <- house_115[,c (1, 5:7)]
house_a

#you can also select specific rows AND specific columns (rows 100 to 200)
house_b <- house_115[100:200, c(1, 5:7)]

#create a df only involving female lawmakers data,
house_f <-house_115[house_115$gender=="F",]

#about the last homework assignment, this house_115 contains all of the data on who left and stuff
#he did it for us. 
#now let's fix it
house_115
house_115_2016<-house_115[!(house_115$successor==1) #take out the successors
                          &!(house_115$non_voting==1),] #take out the non-voting members

nrow(house_115_2016) #now we have 435 rows, so there we go!

house_115_2018 <-house_115[!(house_115$vacate==1) #take out the people who left 
                           &!(house_115$non_voting==1),] #take out the non-voting members

nrow(house_115_2018) #and we have the state of house_115 in 2018

#now calculate gender ratio of the starting setup

length_f_115_2016 <- house_115_2016$gender=='F' 
length(length_f_115_2016)
/ nrow(house_115_2016)
ratio_115_2016

ratio_115_2016 <- sum(house_115_2016$gender=='F') / nrow(house_115_2016)
ratio_115_2016
partisan_ratio_2016 <-length(house_115_2016$party[house_115_2016$party=='R']) / length(house_115_2016$party[house_115_2016$party=='D'])
partisan_ratio_2016


#another assignment
#combine first and last name
house_115_2016$name <-
  paste(house_115_2016$first_name,
        house_115_2016$last_name, sep=(' '))

#add sex, female is 1, male is 0
house_115_2016$sex <- 0
house_115_2016$sex[house_115_2016$gender=="F"] <-1

#party_cate: R is 1, D is 2, and others is 0
house_115_2016$party_cate <- 0
house_115_2016$party_cate[house_115_2016$party=="R"] <-1
house_115_2016$party_cate[house_115_2016$party=="D"] <-2

#year: lawmakers' birth year
house_115_2016$year <- substr(house_115_2016$birth, 1, 4)





#in case we need to calculate teh age
require(lubridate) #use lubricate package, also check out 'zoom', another package.

age<-trunc((birth %--% x_date) / years(1)) #explain what it does!
mean(age) #63.66447








#_____________________________________________________________________________
# lecture 4 

house_115_2016 #435
house_115_2018 #432

#create an extra column with name (it equals first name + last name)
house_115_2016$name<- paste(house_115_2016$first_name, house_115_2016$last_name, sep=' ')
house_115_2016$name

#sex female is 1 and male is 0
#first add 0 to all rows, and then change to 1 if they are female
unique(house_115_2016$gender) #check how many genders in the house
house_115_2016$sex <- 0
house_115_2016$sex[house_115_2016$gender=="F"] <- 1 

#party category. from 0 to 1 and 2. 
unique(house_115_2016$party) # to see what parties are there
#if only two parties you can also give everything 2 and then change 
#to 1 if they are Republican. 
house_115_2016$party_cate <- 0
house_115_2016$party_cate[house_115_2016$party=="R"] <-1
house_115_2016$party_cate[house_115_2016$party=="D"] <-2

#lawmakers' birth year
house_115_2016$year <- substr(house_115_2016$date_of_birth, 1, 4)

#clean data with regexpr, what if there's variations in the date?
y <- c("25-10-30", "1725-9-10", "137-12-17")

#this will show us (in the side tab), where the first - is located for each
#item in the list y. so that will be 3, 5, 4. 
m <- regexpr ("-", y)
#and now we can create new substr from 1, to m-1 (so first symbol until
#the last symbol before the - (as defined by m. so first one that would be
#position 1 until 2, for the second position 1 to 4, and third pos 1 to 3))
year <-substr(y, 1, m -1) # output "25", "1725" "137" Perfect!

#so now let's do it on the house. Luckily, all the first '-' are already
#the same on the fifth position. 
minus <- regexpr ("-", house_115_2016$date_of_birth)
minus
house_115_2016$year<- substr(house_115_2016$date_of_birth, 1, minus -1)
house_115_2016$year

#week 5 lecture and homework______________________________________________________________________

library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(readxl)
#install.packages(writexl)
#library(writexl)

#options(warn = -1)

#now let's try this stuff with the house 
setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/house")


house_115 <- read.csv("house_115.csv")
house_116 <- read.csv("house_116.csv")
house_117 <- read.csv("house_117.csv")

house_115_2016 <- house_115 %>%
  filter(successor != 1) %>%
  filter(non_voting != 1)

house_116_2019 <- house_116 %>%
  filter(successor != 1) %>%
  filter(non_voting != 1)

house_116_2019 <- house_116_2019[!(house_116_2019$name == "Justin Amash" & house_116_2019$party == "I"),]
house_116_2019 <- house_116_2019[!(house_116_2019$name == "Paul Mitchell" & house_116_2019$party == "I"),]
house_116_2019 <- house_116_2019[!(house_116_2019$name == "Jefferson Van Drew" & house_116_2019$party == "R"),]

house_117_2021 <- house_117 %>%
  filter(successor != 1) %>%
  filter(non_voting != 1)


#__________________________________________________________

house_115_g<-house_115_2016%>%
  group_by(gender, party)%>%
  summarise(sex=n(),
            ratio=n()/nrow(.))

#calculate every state's party gender ratio by state, you have to divide it by
#this state's total number of lawmakers, not by the whole dataset. 
#so the answer below was wrong
house_115_stateratio <-house_115_2016%>%
  group_by(gender, party, state)%>%
  summarise(sex=n(),
            ratio=n()/nrow(.))

#nowt the proper one
total_per_state<-house_115_2016%>%
  group_by(state)%>%
  summarise(rep=n())

state_ratio <-house_115_2016%>%
  group_by(party, state, gender)%>%
  summarise(sex=n())

merged_df <-total_per_state%>%
  left_join(state_ratio, by='state')

#and now I just have to divide the occurances of a certain gender per state
#fix the columns of merged_df first
merged_df <- merged_df %>% 
  mutate(sex_ratio = sex/rep)

#_____________________________________________________
setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")
trump <- read_excel("trumpscore.xlsx")

house_115 <- house_115_2016 %>% rename(bioguide = id)

trumpscore <-left_join(trump, house_115, by = c("bioguide"))


#merge them together to couple gender to id / bioguide
tru<-trumpscore%>%
  group_by(gender) %>%
  summarize(agree = mean(agree_pct, na.rm = TRUE))%>% #na.rm is for when data is missing
  filter(!is.na(gender))

#but if we look at female republicans, we see that actually female republicans 
#are more likely to vote in favor of Trump 
ungroup()
tru2<-trumpscore%>%
  group_by(party.x, gender) %>%
  summarize(agree = mean(agree_pct, na.rm = TRUE))%>% #na.rm is for when data is missing
  filter(!is.na(gender))

tru2<- lm(agree_pct~gender+party,
          data=house_115_2016)
summary(tru2)


#GPA_________________________________________
library(ggplot2)
gpa <- read.csv("gpa.csv")


plot(gpa$studyHr, gpa$GPA)

#plot the hours of studying compared to the grades
ggplot(gpa, aes(x=studyHr, y=GPA, color = gender)) + 
  geom_point()

#ok
ggplot(gpa, aes(x = gender))+
  geom_bar()


#different genders different performance in GPA 
gender<-gpa%>%
  group_by(gender)%>%
  summarise(GPA=mean(GPA))

ggplot(gender, aes(gender, GPA))+
  geom_bar(stat="identity") #identity means that there is one value. 

house_115_2016

ggplot(house_115_2016, aes(gender)) + 
  geom_bar()

#create a new dataframe that includes three terms' gender information. 
house_gender_115 <- house_115_2016 %>%
  group_by(gender)%>%
  summarise(number=n())%>%
  mutate(term="115")

house_gender_116 <- house_116_2019 %>%
  group_by(gender)%>%
  summarise(number=n())%>%
  mutate(term="116")

house_gender_117 <-house_117_2021%>%
  group_by(gender)%>%
  summarise(number=n())%>%
  mutate(term="117")

house_gender<-rbind(house_gender_115, house_gender_116, house_gender_117)

# fill=gender is what we need to finish this stuff
ggplot(house_gender, aes(term, number, fill=gender))+
  geom_bar(stat='identity')

ggplot(house_gender, aes(term, number, fill=gender))+
  geom_bar(stat='identity', position= position_dodge())

ggplot(house_gender, aes(term, number, group=gender))+
  geom_line(aes(color=gender))


#___________________________________________________________

#percentage of ppl voting against their party
party_vote <- data.frame(term=c('115', '116', '117'),
                         against_party=c(mean(house_115_2016$votes_against_party_pct),
                                         mean(house_116_2019$votes_against_party_pct, na.rm=TRUE),
                                         mean(house_117_2021$votes_against_party_pct)))
party_vote 
ggplot(party_vote, aes(x = term, y = against_party, group=1)) +
  geom_line()

#now what if we wanna compare men and women so we need a few 
#create different data frames
house_gender_115<-house_115_2016%>%
  group_by(gender)%>%
  summarise(number=n())%>%
  mutate(term='115')

party_vote_115<-house_115_2016%>%
  group_by(gender)%>%
  summarise(against_party=mean(votes_against_party_pct))%>%
  mutate(term="115")

party_vote_116<-house_116_2019%>%
  group_by(gender)%>%
  summarise(against_party=mean(votes_against_party_pct, na.rm = TRUE))%>%
  mutate(term="116")

party_vote_117<-house_117_2021%>%
  group_by(gender)%>%
  summarise(against_party=mean(votes_against_party_pct))%>%
  mutate(term="117")

#rbind
party_vote_all <- rbind(party_vote_115, party_vote_116, party_vote_117)

#now we need to add the total against party, we made it earlier
#put gender all to the old df, 
party_vote$gender='all'

#bind back together
party_vote_all <-rbind(party_vote_all, party_vote)

#plot this one doesn't work lol
ggplot(party_vote_all, aes(x=term, y=against_party, group=gender)) +
  geom_line(aes(color=gender))


#lecture 6_______________________________________________________________________________
#Class: Week 06
#Course: Big Data and Social Analysis
#Semester: Spring 2023
#Lesson: Loop and Advanced Dataframe Manipulation
#Instructor: Chung-pei Pien
#Organization: ICI, NCCU

### Student Information --------

#Chinese Name: 嘉博
#English First Name:Jasper
#UID: 111926019 
#E-mail: jasper.hewitt@me.com 

### Questions --------


#In this week's class, you utilized loop to create ntc_ref_2018.

#Download ntc_2021.zip and extract it into your working directory.
setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")


#These files contain the results of New Taipei City's 2021 pro-nuclear power referendum (Case 17) at the li level.
#The content of these files differs from 17_65000.json, as they have been cleaned.
#The two referendums had notably different outcomes: the pro-nuclear side won in 2018 but lost in 2021.
#We are interested in examining the differences between the two referendums at both the district and li levels.
#Please answer the following questions. Remember, No Comments, No Points!!!!!!!

#_______________________________________________________________________________________________________
#Question 1: (10 points) Use loop function to rbind all districts’ 2021 election results into ntc_ref_2021

#list all the files from ntc_2021
ntc_2021_list<-list.files('ntc_2021')
ntc_2021_list[1]

#create an empty data frame called ntc_ref_2021
ntc_ref_2021<- data.frame() #empty dataframe

#read every item in the ntc_2021_list[i] (total 29) and store it in the temp_df variable
#rbind temp_df to ntc_ref_2021. 
#note that the values in temp_df will be overwritten in the next iteration of the loop. 
for (i in 1:29){
  temp_df<-read.csv(paste("ntc_2021/",ntc_2021_list[i], sep=""))
  ntc_ref_2021<-rbind(ntc_ref_2021, temp_df)
}
ntc_ref_2021 #1032 obs. of 8 variables

#_______________________________________________________________________________________________________
#Question 2: (3 points) Merge ntc_ref_2018 and ntc_ref_2021 into ntc_ref

#step 1: do the same for 2018 
library(dplyr)
ntc_2018_list<-list.files('ntc_2018')
ntc_2018_list[1]

ntc_ref_2018<- data.frame() 
for (i in 1:29){
  temp_df<-read.csv(paste("ntc_2018/",ntc_2018_list[i], sep=""))
  ntc_ref_2018<-rbind(ntc_ref_2018, temp_df)
}

#step 2: delete excess columns from ntc_ref_2018 (because this is a comparison I will only take the 
#columns that are also in ntc_ref_2021)
#see columns ntc_ref_2021
colnames(ntc_ref_2021)

#only select relevant comlumns in ntc_ref_2018
ntc_ref_2018 <- ntc_ref_2018[c("district","li_id", "rf_16_yea", "rf_16_nay", "rf_16_valid_vote",
                               "rf_16_invalid_vote", "rf_16_turnout", "rf_16_num_voter")]

#step3: merge the two columns together on li_id and drop the duplicate district.y column 
ntc_ref <-merge(ntc_ref_2018, ntc_ref_2021, by='li_id')
ntc_ref <- subset(ntc_ref, select = -district.y)

#_______________________________________________________________________________________________________
#Question 3: (3 points) Using dplyr's mutate to calculate every li's yea rate in 2018 and 2021 (number of yea / number of valid vote)

#create a new column yea_rate_2018 that divides the 2018 number of yeas by the valid votes in 2018
ntc_ref <- ntc_ref %>% 
  mutate(yea_rate_2018 = rf_16_yea/rf_16_valid_vote)

#do the same for 2021
ntc_ref <- ntc_ref %>% 
  mutate(yea_rate_2021 = rf_17_yea/rf_17_valid_vote)

#_______________________________________________________________________________________________________
#Question 4: (2 points) Which li's yea rate has largest decrease? (Give me the li_id)
#calculate the difference between yea_rate in 2018 and 2021.
ntc_ref <- ntc_ref %>% 
  mutate(yea_change = yea_rate_2021 - yea_rate_2018)

#sort the df in an ascending order to find the biggest decrease (this will show the row with the biggest minus value first)
ntc_ref_sortdecrease <- ntc_ref[order(ntc_ref$yea_change, decreasing =FALSE),]
#show first row with head
head(ntc_ref_sortdecrease, 1) #li_id = 6500600-021, decrease = -0.379371

#_______________________________________________________________________________________________________
#Question 5: (2 points) Which li's yea rate has largest increase? (Give me the li_id)

#sort the df descending based on the yea_change column. So showing the highest value first  
ntc_ref_sortincrease <- ntc_ref[order(ntc_ref$yea_change, decreasing =TRUE),] 
#show first row with head()
head(ntc_ref_sortincrease, 1) #li_id:6500400-010, increase = +0.2155448


#_______________________________________________________________________________________________________
#Question 6: (6 points) Using dplyr's group_by() and summarise() to calculate every district's yea rate
#group_by district, use summarize to divide the sum of 2018 yeas per district by the sum of 2018 valid votes
district_yea_rate_2018<-ntc_ref%>%
  group_by(district.x)%>%
  summarise(district_yea_2018=sum(rf_16_yea) / sum(rf_16_valid_vote))

#now do the same for 2021
district_yea_rate_2021<-ntc_ref%>%
  group_by(district.x)%>%
  summarise(district_yea_2021=sum(rf_17_yea) / sum(rf_17_valid_vote))

#merge the two dataframes together
district_yea_rate<-merge(district_yea_rate_2018, district_yea_rate_2021, by='district.x')
#_______________________________________________________________________________________________________
#Question 7: (2 points) Which district's yea rate has largest decrease?
#use same code as in question 4 to find the largest decrease

#calculate the difference between mean yea rate in 2018 and 2021.
district_yea_rate <- district_yea_rate %>% 
  mutate(yea_change = district_yea_2021 - district_yea_2018)

#sort the df in an ascending order based on the yea_change column to find the biggest decrease.
district_yea_rate_decrease <- district_yea_rate[order(district_yea_rate$yea_change, decreasing =FALSE),]

#show first row with head
head(district_yea_rate_decrease, 1) #Gongliao, decrease -0.25320491

#_______________________________________________________________________________________________________
#Question 8: (2 points)

#Which district's yea rate has largest increase? (NO DISTRICT WITH INCREASE)

#sort the df descending based on the yea_change column. So showing the highest value first
district_yea_rate_increase <- district_yea_rate[order(district_yea_rate$yea_change, decreasing =TRUE),]

#show first row with head
head(district_yea_rate_increase, 1) #Xindian, decrease -0.01397470

#answer: there is no district with a mean increase in yeas. 
#all districts show a decrease in yea rates. 
#the district with the smallest decrease in yeas is Yonghe. 



#_______________________________________________________________________________________________________


install.packages("jsonlite")
library(jsonlite)

a<-3
for (i in 1:10) {
  a<-a+i
}

ici <- c('nccu', 'taiwan', 'best', 'lovely')
for (i in 1:length(ici)){
  print(ici[i])
}


setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")

house_115_json <- fromJSON("congress-115.json")

house_115_json

#let's look at the referendum about nuclear power
ntc_ref_json <-fromJSON('17_65000.json')

ntc_ref_1 <- ntc_ref_json[[1]]
ntc_ref_2 <- ntc_ref_json[[2]]

ntc_ref_2021<-rbind(ntc_ref_1, ntc_ref_2)

#but we can also do this with a loop
ntc_ref_2021 <- data.frame() #empty dataframe
for(i in 1:29){
  temp_df<-ntc_ref_json[[i]]
  ntc_ref_2021<-rbind(ntc_ref_2021, temp_df)
}
#you can play around with the i to see if the code worked. But run it
#from the second line in the loop, not from the start of the i loop. 
i<-5
ntc_ref_2021

# now what if they give you a bunch of csv of xlsx files instead of a json 

setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")

ntc_2018_list<-list.files('ntc_2018')
ntc_2018_list[1]

ntc_2018_1 <- read.csv(paste0("ntc_2018/",ntc_2018_list[1]))

ntc_ref_2018<- data.frame() #empty dataframe
for (i in 1:29){
  temp_df<-read.csv(paste("ntc_2018/",ntc_2018_list[i], sep=""))
  ntc_ref_2018<-rbind(ntc_ref_2018, temp_df)
}




ntc_ref_2018


#week 7 and homework_________________________________________________________________
#Class: Week 07
#Course: Big Data and Social Analysis
#Semester: Spring 2023
#Lesson: Regular Expression
#Instructor: Chung-pei Pien
#Organization: ICI, NCCU

### Student Information --------

#Chinese Name: 嘉博
#English First Name: Jasper 
#UID: 111926019 
#E-mail: jasper.hewitt@me.com

### Questions --------

#Please read "trumptweets.xlsx", which contains Trump's tweets from Feb. 2017 to May 2018.
#The "text" column contains the content of Trump's tweets.
library("readxl")
library("dplyr")
library('stringr')
#Question 1 (5 points):

#Using regular expressions (regexpr(), grep(), or grepl()) and other R skills, 
#create a new column called "china".
#Label "china" column as 1 if Trump mentioned China, otherwise label it as 0.
setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")
trumptweet <- read_xlsx("trumptweets.xlsx")

#since trump likes to write in caps, we first convert everything to lower case
trumptweet$text <- tolower(trumptweet$text)

#create a new column called China where all the values are NA
trumptweet$China <- NA  

# I decided to use stringr from the Regex cheat sheet you provided because it looks a lot like
#str.contains in python. str_detect will return an output of TRUE and FALSE to show whether
#the sentence does or does not contain the word 'china'. I store the output 
#in the variable 'chinamatch'
chinamatch <- str_detect(trumptweet$text, "china")

# if chinamatch for a specific row is TRUE, assign 1 to that row in the China column 
trumptweet$China[chinamatch] <- 1
#  if chinamatch for a specific row is FALSE, assign 0 to that row in the China column
trumptweet$China[!chinamatch] <- 0

#count in how many tweets the word China appeared (count how many times 
#1 appears in the new column)
china_count <- sum(trumptweet$China == 1)
print(china_count) #73 tweets that contain the word 'china'

#_______________________________________________________________________________
#Question 2 (3 points):

#The "created_at" column indicates when Trump posted his tweets.
#Use "created_at" column to create a new column called "year", which shows the 
#year in which the tweet was made.

# find the position of the first '-' in the 'created_at' column and store it in the variable yp
yp <- regexpr('-', trumptweet$created_at)

#extract the characters from from the created_at column. 
#Only take the ones from the first position
#until the position of the first '-' minus 1 (because we do not want to include the dash).
#this basically takes the first 4 characters from the rows in the 
#created_at column and assigns them to the
#new column 'year'
trumptweet$year <- substr(trumptweet$created_at, 1, yp-1)

#_______________________________________________________________________________
#Question 3 (4 points):

#Use group_by() and summarise() to determine the number of tweets 
#made by Trump that mentioned China in 2017 and 2018.

#step 1: select the two relevant columns, do not consider the rest
#step 2: group_by year. so the new df will have two rows, 2017 and 2018 
#step 3: use summarize to create a new column 'china_per_year' with 
#the sum of the 1s China column per year. 
#because all the values in the China column are already 0 or 1, this is an easy step
selected_tweets <- trumptweet%>%
  select(China, year)%>%
  group_by(year)%>%
  summarize(china_per_year=sum(China)) #2017: 43, 2018: 30

#_______________________________________________________________________________
#Question 4 (3 points):

#The "is_retweet" column represents whether a tweet is a retweet or not.
#Create a new dataframe called "notrt_trump" that includes only Trump's original 
#tweets (i.e., tweets that are not retweets).

#use dplyr's 'filter' to filter trumptweet based on the condition is_retweet == 
#FALSE (not a retweet).
#the new dataframe is stored in notr_trump. this dataframe only contains 
#Trump's original tweets
notrt_trump <- trumptweet %>%
  filter(is_retweet == FALSE) #2845 observations

#_______________________________________________________________________________
#Question 5 (5 points):

#On many social media platforms, the "@" symbol is referred to as a "handle".
#Apply regular expressions to extract the username associated with the "@" symbol 
#in the "text" column of "notrt_trump" for the first row.

#there is two ways to solve this problem. If we wanna stick to the same 
#structure as we did in last class
#we can do the following: 
#find the starting position and length of the Twitter handle 
#we can use the regexpr pattern that we used for the hashtags last class.
#but we replace the # with @. The regexpr gets patterns that start with an @
#and are followed by any letters or numbers, dashes and underscores. the plus 
#the end means that it has to have at least one character after the @ in order 
#to be selected by the regexpr. this is to prevent us from capturing any random
#instances of the symbol '@'
user_start_pos <- regexpr("@[A-Za-z0-9_]+", notrt_trump$text[1])
print(user_start_pos) #starting position 226. length 11 

#extract substring. from starting position, starting position + the length 
#of the handle -1 (to get rid of the space)
username <- substr(notrt_trump$text[1],
                   user_start_pos,
                   user_start_pos + attr(user_start_pos, "match.length")-1)
print(username) #@nikkihaley

#alternatively, I found a simpler and faster method on stack overlflow that uses regmatches. 
#regmatches is also on the cheat sheet: regmatches(string, gregexpr(pattern, string))
#this directly extracts the pattern
handle <- regmatches(notrt_trump$text[1], gregexpr("@[A-Za-z0-9_]+", notrt_trump$text[1]))
print(handle) #@nikkihaley

#_______________________________________________________________________________
#Question 6 (3 points)

#Use gregexpr() to obtain a list called "atsign", which shows the starting positions 
#and length of the usernames associated with the "@" symbol in the "text" column.

#we can use the same code as above and what we used in class. But I add perl=TRUE 
#to get the pattern. 
atsign <- gregexpr("@[A-Za-z0-9_]+", notrt_trump$text)

print(atsign)

#Review the "atsign" list and identify the smallest row number that contains 
#"@" symbols with a length greater than 2 and tell me the row number, ANSWER row 159

#I assume you mean the first row that has more than two handles.
#we can scroll through the list to find the first hit, but this is time consuming. 
#I found some code that uses which() and lengths() to find the 
#lengths(atsign)>2 returns TRUE/FALSE for every element in the list. if the element in the list 
#has more than 2 starting positions (and thus twitter handles) it shows 
#TRUE, otherwise it shows FALSE
#I also use which() so that the output only contains the elements that are 
#TRUE so that I don't have to
# manually scroll through the output to find the first position in the list that 
#has more than 2 starting positions

which(lengths(atsign) >2) #first row: 159
#_______________________________________________________________________________
#Question 7 (7 points)

#Using a loop, extract the usernames associated with the "@" symbol in the 
#"text" column of "notrt_trump" for the row you identified in Question 6.

#I've decided to look at row number 159.

#for x in 1 to the length of atsign[[159]] (row number 159)
for (x in 1:length(atsign[[159]])) {
  
  #at_content = extract from the starting positions of the handles in the text column of row 8 
  #+ the length of said handles -1 (to get rid of the space)
  at_content <- substr(notrt_trump$text[159],
                       atsign[[159]][x],
                       atsign[[159]][x] + attr(atsign[[159]], "match.length")[x] - 1)
  
  print(at_content) #"@flotus" "@emmanuelmacron" "@whitehouse"
}

#alternatively, the code below using regmatches shows the same results
#however, this approach does not use a loop so it does not meet the requirements of the
#question. 
#I am curious what you think: Are there any 優點 to using the code above as opposed to the
#one below? Do you think the one below may not be able to extract handles under some 
#circumstances?
handle_159 <- regmatches(notrt_trump$text[159], gregexpr("@[A-Za-z0-9_]+", notrt_trump$text[159]))
print(handle_159) #"@flotus" "@emmanuelmacron" "@whitehouse"


#Now you have 30 points! Question 8 is a bonus!
#_______________________________________________________________________________
#Question 8 (5 points)

#Using loops, extract the usernames associated with the "@" symbol in the 
#text column of "notrt_trump" for all rows.

#we can put the code above in another loop that goes through all the rows. 
#And store the extracted usernames in a df
#we can use parts of the code from last lecture 
#create empty df
df <- data.frame()
#for every row in the notr_trump df, use gregexpr to get the starting positions 
#and length of the handles
for (i in 1:nrow(notrt_trump)) {
  atsign <- gregexpr("@[A-Za-z0-9_]+", notrt_trump$text[i], perl=TRUE)
  
  #here we do the same as before. However, this time we replace substr(notrt_trump$text[8], 
  #with substr(notrt_trump$text[i],
  #this makes the code loop through all the sentences and extract 
  #the hasthags from all of them
  #this is done in the same way. #at_content = the starting positions of the 
  #handles in the text column of row 8 
  #+ the length of said handles -1 (to get rid of the space)
  for (x in 1:length(atsign[[1]])) {
    at_content <- substr(notrt_trump$text[i],
                         atsign[[1]][x],
                         atsign[[1]][x] + attr(atsign[[1]], "match.length")[x] - 1)
    #store output in a temporary df. 
    temp <- data.frame(row_id = i,
                       usernames = at_content)
    #bind the temporary df to the main df after every iteration of the loop
    df <- rbind(df, temp)
  }
}


#[EXTRA POSSIBILITY] alternatively, we can use regmatches toget the same results.
#the following returns a list that shows the handles per index number
#I am aware that this alternative solution does not use a loop
#just putting it here as a little extra side possibility
all_handle <- regmatches(notrt_trump$text, gregexpr("@[A-Za-z0-9_]+", notrt_trump$text))



#regex and loops 
library(dplyr)
library(ggplot2)
install.packages("readxl")
library(readxl)


re1 <- data.frame(id = 1:12,
                  birth = c("1978Y11M", "1968Y1M", "1828Y10M", "1967Y5M", "1717Y4M", 
                            "1948Y12M", "1952Y06M", "1828Y10M", "1927Y3M", "1854Y6M",
                            "287Y6M", "19Y10M"))

yp<-regexpr('Y', re1$birth) #find in what location all the Y's are, most in 5

#write down the year, based on the yp we just created 
re1$year <-substr(re1$birth, 1, yp -1)

#find and create the month based on the yp we just created. 
re1$month <-substr(re1$birth, yp+1, nchar(re1$birth)-1)
#__________________________________________________________________________________


re2 <- data.frame(id = 1:13,
                  birth = c("1978Y11M", "1968Y1M", "1828Y10M", "1967Y5M", "1717Y4M", 
                            "1948Y12M", "1952Y06M", "1828Y10M", "1927Y3M", "1854Y6M",
                            "287Y6M", "19Y10M", "1921y12M"))
yp <-regexpr('Y', re2$birth) #this time there is one with a small y. how to solve?

#we can tell R that the pattern is Y or y
yp <- regexpr("Y|y", re2$birth)


#I don't see how he finished this, ask later

#__________________________________________________________________________________
re3 <- data.frame(nameid = c("Mary001", "Ben1029", "Billy6587", "John21000", "Jane410", 
                             "Max2946", "Catherine0358", "Eva9863", "Adam212", "Tracy1215"),
                  birth = c("1978Y11M", "1968Y1M", "1828Y10M", "1967Y5M", "1717Y4M", 
                            "1948Y12M", "1952Y06M", "1828Y10M", "1927Y3M", "1854Y6M"))

np<-regexpr("[A-Za-z]*", re3$nameid, 
            attr(np, "match.length") #match length of all the letters
        
print(np)
#now do the re and get new column 
re3$name <- substr(re3$nameid, np,
                   attr(np, "match.length"))
            
#get id
re3$id<- substr(re3$nameid, attr(np, 'match.length')+1, nchar(re3$nameid))
            
            

#Practice 1 do the same for this one 

ex_re3 <- data.frame(nameid = c("001Mary", "1029Ben", "6587Billy", "21000John", "410Jane", 
                                "2946Max", "0358Catherine", "9863Eva", "212Adam", "1215Tracy"),
                     birth = c("1978Y11M", "1968Y1M", "1828Y10M", "1967Y5M", "1717Y4M", 
                               "1948Y12M", "1952Y06M", "1828Y10M", "1927Y3M", "1854Y6M"))

#now do the same that it is turned around. lol I would still have to practice
#this. 
np <- regexpr("\\d+", ex_re3$nameid) 
ex_re3$id <- substr(ex_re3$nameid, np, attr(np, "match.length"))


ex_re3$name <- substr(ex_re3$nameid, attr(np, 'match.length')+1, nchar(re3$nameid))

print(np)

#Torrent's answers. Almost the same as mine,, basically the same lol
z <-regexpr('[0-9]*', ex_re3$nameid)
attr(z, "match.length")
ex_re3$id <- substr(ex_re3$nameid, z, attr(z, 'match.length'))
ex_re3$name<-substr(ex_re3$nameid, attr(z, 'match.length')+1, nchar(re3$nameid))

#

#___________________________________________________________________________________
re4 <- data.frame(nameid = c("MaRy001", "Ben1029", "billy6587", "JoHn21000", "Jane410", 
                             "max2946", "Catherine0358", "Eva9863", "aDAm212", "Tracy1215"),
                  birth = c("1978Y11M", "1968Y1M", "1828Y10M", "1967Y5M", "1717Y4M", 
                            "1948Y12M", "1952Y06M", "1828Y10M", "1927Y3M", "1854Y6M"))


#split both name from id and year from month
np<-regexpr("[A-Za-z]*", re4$nameid) #get length of the letter, I still don't really get why we do this
            #attr(np, "match.length") #match length of all the letters
            
#now do the re and get new column 
re4$name <- substr(re4$nameid, np,attr(np, "match.length"))

#this one is not really working yet       
re4$name_fix <- paste0(toupper(substr(re4$name, 1, 1)), tolower(substr(re4$name, 2, nchar(re4$name))

                                                                 #get id
re4$id<- substr(re4$nameid, attr(np, 'match.length')+1, nchar(4$nameid))



#________________________________________________________________________________

re5 <- data.frame(id = 1:12,
                  birth = c("1978[11]", "1968[1]", "1828[10]", "1967[5]", "1717[4]", 
                            "1948[12]", "1952[06]", "1828[10]", "1927[3]", "1854[6]",
                            "287[6]", "19[10]"))
yp <- regexpr ('[',re5$birth) #this one has problems?
yp <- regexpr('\\[', re5$birth)
#this one
re5$year <- substr(re5$birth, 1, yp-1)
re5$month <- substr(re5$birth, yp +1, nchar(re5$birth)-1)

#Practice 2

ex_re5 <- data.frame(id = 1:12,
                     birth = c("1978(11)", "1968(1)", "1828(10)", "1967(5)", "1717(4)", 
                               "1948(12)", "1952(06)", "1828(10)", "1927(3)", "1854(6)",
                               "287(6)", "19(10)"))

#________________________________________________________________________________
#ukraine dataset
setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")
ua_example <- read_xlsx("UkraineTweets_example.xlsx")


#_______________________________________________________________________________
#use gregexpr to select all of the hashtags 
ht <-gregexpr("#[a-zA-Z0-9]+", ua_example$text) # we wanna get all the texts

#code from torrent
df <- data.frame()

#the code below basically tdoes this 
#substr(ua_example$text[1], 14, 14 + 7 - 1) (same formula, start of the hashtag
# plus length of the word minus 1)!!! 
ht[[2]][2] #play around with this to see the positions of the hashtags

#just explanation of the first sentence! same as above but then with 1:5 bc it
#is only 5 hashtags 

  
ht_content <- substr(ua_example$text[1],
                     ht[[1]][x],
                     ht[[1]][x] + attr(ht[[1]], "match.length")[x] - 1)
  
print(ht_content)
  


#but if we want to look at all the posts we need a different kind of loop with (x)
#see the one above. 
for (x in 1:length(ht[[1]])) {
  #so the code here is figuring out both the starting position of the hashtag
  #as well as the number of letters to the next hashtag. that way you can grab all
  #of them that's why we use match.length to match the length of the word
  ht_content <- substr(ua_example$text[1],
                       ht[[1]][x],
                       ht[[1]][x] + attr(ht[[1]], "match.length")[x] - 1)
  
  temp <- data.frame(id = x,
                     hashtag = ht_content)
  
  df <- rbind(df, temp)
  
}

#but if we want to look at all the posts we need a different kind of loop with (x)
#see the one above. 
for (x in 1:nrow(ua_example))  {
  #so the code here is figuring out both the starting position of the hashtag
  #as well as the number of letters to the next hashtag. that way you can grab all
  #of them that's why we use match.length to match the length of the word
  ht_content <- substr(ua_example$text[1],
                       ht[[1]][x],
                       ht[[1]][x] + attr(ht[[1]], "match.length")[x] - 1)
  
  temp <- data.frame(id = x,
                     hashtag = ht_content)
  
  df <- rbind(df, temp)
  
}

#chat gpt finished the code for making it run through everything
# Define a regular expression to match hashtags
regex <- "#[a-zA-Z0-9]+"

# Create an empty dataframe to store the hashtag information
df <- data.frame()

# Loop over each row of the dataframe and extract the hashtags
for (i in 1:nrow(ua_example)) {
  # Use gregexpr to find the starting position of the hashtags
  ht <- gregexpr(regex, ua_example$text[i], ignore.case = TRUE)
  
  # Loop over each hashtag in the row and extract its content
  for (x in 1:length(ht[[1]])) {
    # Use substr to extract the hashtag content
    ht_content <- substr(ua_example$text[i],
                         ht[[1]][x],
                         ht[[1]][x] + attr(ht[[1]], "match.length")[x] - 1)
    
    # Create a dataframe with the hashtag information and append it to the main dataframe
    temp <- data.frame(row_id = i, hashtag = ht_content)
    df <- rbind(df, temp)
  }
}

# Print the resulting dataframe
print(df)

#week8_______________________________________________________________________________________________

library(readxl)
setwd("/Users/jasperhewitt/Desktop/big data & social analysis/code/datasets")
trumptweet <- read_xlsx("trumptweets.xlsx")
library(readr)

#tidyr gather) and spread()
library(tidyverse)
g_w <- data.frame(country = c("Afghanistan", "Brazil", "China"),
                  `1999` = c(745, 37737, 212258),
                  `2000` = c(2666, 80488, 213766), check.names=FALSE)
#now what if we want to transfer it into a long table, use gather()
g_l <- g_w %>%
  gather('year', 'cases', 2:3) #year, and cases are the new column names
#2:3 stands for the columns that you are going to gather () from the df
#2 contains the the numbers for 1990 and 3 contains the data for 2000
#they will be gathered () together under the cases column 

#then we can plot it
ggplot(g_l, aes (x=year, y=cases, color=country,
                 group=country))+
  geom_line()


s_l <- data.frame(country = c("Afghanistan", "Afghanistan", "Afghanistan", "Afghanistan", 
                              "Brazil", "Brazil","Brazil","Brazil", 
                              "China","China","China","China"),
                  year = c(1999, 1999, 2000, 2000, 1999, 1999, 2000, 2000, 1999, 1999, 2000, 2000),
                  key = c("cases", "population", "cases", "population",
                          "cases", "population", "cases", "population",
                          "cases", "population", "cases", "population"),
                  value = c(745, 19987071, 2666, 20595360, 37737, 172006362,
                            80488, 174504898, 212258, 1272915272, 213766, 1280428583))
#this one is already a long table. but we need to spread it into a wide table
#use function spread()
s_w <- s_l %>%
  spread(key, value) #ok this was actually pretty straightforward lol 








#as.Date lets you put things in a timeformat

#number of tweets per day
trumptweet <- read_xlsx("trumptweets.xlsx")

#as.Date will transfer it into the regular time format of R
trumptweet$date <- as.Date(trumptweet$created_at)


#get the tweets for june 2017
trump_june17 <- trumptweet%>%
  filter(date >= '2017-06-01' & date <= '2017-06-30')

trump_date <- trump_june17 %>%
  group_by(date)%>%
  summarise(post=n())

#line graphs for quantity of posts per day in the month june
library(ggplot2)
ggplot(trump_date, aes (x = date, y=post))+
  geom_line()

#google mobility data 

google_mobility <- read.csv("google_mobility_data.csv")

google_mobility$date <- as.Date(google_mobility$date)

#filter by taiwan, date, and only select date and transit.stations
google_mobility_taiwan <- google_mobility%>%
  filter(country_region == "Taiwan")%>%
  filter(date >= '2021-01-01' & date <= '2021-05-18')%>%
  select(date, transit.stations)

#plot
ggplot(google_mobility_taiwan, aes (x=date, y=transit.stations))+
  geom_line()


#what if you have several differnt type of dates in one df

date_4 <- ('20150407')
# so the capital y stands for a date with a century denomination. 
date_4 <- as.Date("20150407", "%Y%m%d") #what is up with the capital. idk??

#own experiment. not working lol. maybe to messed up
date <- '1543'
date1<-as.Date(date, "%Y%m%d")

date_6<-as.Date("11022012") #this will lead to an error
date_6<-as.Date("11022012", "%m%d%Y") #this will lead to a proper format

date_6<-date_6<-as.Date("11-02-2021", "%m-%d-%y")

date_8<-as.Date("Dec022012") #error
date_8<-as.Date("Dec022012", "%b%d%Y") #this works, doesn't work if your
#system is set to Chinese. it won't recognise Dec


#now back to Google's mobility data. Plot all of the things in the dataframe
#first reshape the df so that it has all the in the rows
#we need new package, tidyr, and use gather() and spread() function
#see examples at the start of this code.
library(tidyr)
google_mobility_taiwan <- google_mobility%>%
  filter(country_region == "Taiwan")%>%
  filter(date >= '2021-01-01' & date <= '2021-05-18')

tw_google_l <- google_mobility_taiwan [, -c(1:4)] %>% #omit the columns we don't need
  gather('move', 'value', 2:7) #select columns 2 to 7
#idk why we first omitted the columns we don't need, we can also just select the columns
#we need right? No. it is better to just drop the columns you don't need
#otherwise the resulting df will be a bit chaotic. 

#now plot it
ggplot(tw_google_l, aes(x=date, y=value, group=move, color=move))+
  geom_line()

#you can change your systems date time if you want

time_date_1 <- as. POSIXct('2015-04-07 10:15:30')
time_date_2 <- as.POSIXct("2015-04-07 101530")
time_date_3 <- as.POSIXct("20150407 101530")

time_date_3 <- as.POSIXct("20150407 101530", format="%Y%m%d %H%M%S")

Sys.timezone() #system timezone

time_date_4 <- as.POSIXct("2015-04-07 10:15:30", tz='UTC')#adjust timezone

trumptweet$time <- 
  as.POSIXct(trumptweet$created_at) #this is just the same as created_at?

#now we just want the hour
trumptweet$hour<-
  format(trumptweet$time, "%H")

trump_hour <-trumptweet%>%
  group_by(hour)%>%
  summarise(post=n())%>%
  mutate(hour_rate=post/sum(post))

#plot 
#and now we group 
ggplot(trump_hour, aes(x=hour, y=post, group=1))+
  geom_line()


#see midterm exam notice. no chatgpt. we don't have to write comments!!!
#download as .r file. 

i <-1
for (i in 1:1000){
  print(i)
  i<- i+1
}

#as.Date and #as.POSIXct
install.packages('zoo')
library(zoo)
#this can make something into month format 
as.yearmon(as.Date("2015-04-07"))


#we also have date to cquarter

date_convert <- data.frame(Date = c("2014-02-05", "2014-08-11", "2014-11-23"),
                           Arrivals = c(100, 150, 200))

#date to month
date_convert$month<-as.yearmon(date_convert$Date)

#date to quarter
date_convert$quarter <- as.yearqtr(date_convert$Date) #only shows 1, weird 
date_convert$quarter <- as.yearqtr(date_convert$month) #when we do it based on 
# the months, it works. because these are in R dataformat, we did that above

#create plot for trump's tweets per month
trumptweet <- read_xlsx("trumptweets.xlsx")
trumptweet$ym<-as.yearmon(as.Date(trumptweet$created_at)) # convert date into
#yearmonth data

trump_ym <- trumptweet %>%
  group_by(ym)%>%
  summarise(post=n())

#plot
ggplot(trump_ym, aes(x=ym, y=post, group=1))+
  geom_line()

#now the quarterly numbers
#create plot for trump's tweets per month

trumptweet <- read_xlsx("trumptweets.xlsx")
trumptweet$ym<-as.yearmon(as.Date(trumptweet$created_at)) # convert date into
#yearmonth data

trumptweet$yq<-as.yearqtr(as.Date(trumptweet$created_at)) # we can just do it form Date directly! no need 
#to go yearmonth first 
#group by and summarise
trump_yq <- trumptweet %>%
  group_by(yq)%>%
  summarise(post=n())

#plot
ggplot(trump_yq, aes(x=yq, y=post, group=1))+
  geom_line()























































































